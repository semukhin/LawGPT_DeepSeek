"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ.
–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google Custom Search API –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.
"""
import requests
import urllib3
import logging
import asyncio
import time
from typing import List, Dict, Any, Optional, Union
from urllib.parse import urlparse
import os
import sys
from dotenv import load_dotenv

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ third_party –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞ shandu
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
THIRD_PARTY_DIR = os.path.join(BASE_DIR, "third_party")
if THIRD_PARTY_DIR not in sys.path:
   sys.path.insert(0, THIRD_PARTY_DIR)

from app.utils import get_url_content
from third_party.shandu.scraper import WebScraper, ScrapedContent

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# –ö–ª—é—á–∏ Google Custom Search
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY", "")
GOOGLE_CX = os.environ.get("GOOGLE_CX", "")

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Å—ã–ª–æ–∫
MAX_SEARCH_RESULTS = 10  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞
MAX_SCRAPE_RESULTS = 5   # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞ (–¢–û–ü-5)

# –í –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞
logging.info(f"Google API Key –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {'–î–∞' if len(GOOGLE_API_KEY) > 0 else '–ù–µ—Ç'}")
logging.info(f"Google CX –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {'–î–∞' if len(GOOGLE_CX) > 0 else '–ù–µ—Ç'}")

# –°–æ–∑–¥–∞–µ–º –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä WebScraper
_scraper = None


def get_scraper() -> WebScraper:
   """
   –ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä WebScraper –∏–∑ Shandu.
   
   Returns:
       WebScraper: –≠–∫–∑–µ–º–ø–ª—è—Ä —Å–∫—Ä–∞–ø–µ—Ä–∞ Shandu
   """
   global _scraper
   if _scraper is None:
       # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä WebScraper –∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –µ–≥–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
       _scraper = WebScraper(
           user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
           respect_robots=False)
       logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω WebScraper –∏–∑ Shandu")
   return _scraper


def google_search(query: str, logs: list, max_results: int = MAX_SEARCH_RESULTS) -> list:
   """
   –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É —á–µ—Ä–µ–∑ Google Custom Search API –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤–µ–±-—Å—Å—ã–ª–æ–∫.
   
   Args:
       query (str): –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
       logs (list): —Å–ø–∏—Å–æ–∫ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ö–æ–¥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.
       max_results (int): –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
       
   Returns:
       list: –°–ø–∏—Å–æ–∫ URL, –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–æ –∑–∞–ø—Ä–æ—Å—É.
   """
   # –£–±–∏—Ä–∞–µ–º –ª—é–±—ã–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∫ –∑–∞–ø—Ä–æ—Å—É —Ç–∏–ø–∞ "–∑–∞–∫–æ–Ω —Å—É–¥–µ–±–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞" –∏–ª–∏ "–ø–æ—Å–ª–µ–¥–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è"
   clean_query = query.replace(" –∑–∞–∫–æ–Ω —Å—É–¥–µ–±–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞", "").replace(" –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è", "")
   
   url = "https://www.googleapis.com/customsearch/v1"
   params = {
       "key": GOOGLE_API_KEY,
       "cx": GOOGLE_CX,
       "q": clean_query,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
       "num": min(max_results, 10),  # Google API –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
       "safe": "active"
   }
   
   logs.append(f"üîç –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ Google Custom Search API –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{clean_query}'")
   logging.info(f"üîç google_search: –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ –¥–ª—è '{clean_query}'")
   start_time = time.time()

   # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–ª—é—á–µ–π
   if not GOOGLE_API_KEY or not GOOGLE_CX:
       logging.warning("‚ùå google_search: API –∫–ª—é—á–∏ Google –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã!")
       logs.append("‚ùå API –∫–ª—é—á–∏ Google –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã!")
       return []

   
   try:
       logging.info(f"üîç google_search: –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ API")
       response = requests.get(url, params=params, timeout=15)
       status_code = response.status_code
       logging.info(f"üîç google_search: –°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞: {status_code}")
       
       response.raise_for_status()
       data = response.json()
       
       # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
       if "items" not in data:
           logging.warning(f"‚ö†Ô∏è google_search: API –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
           logs.append(f"‚ö†Ô∏è Google –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{clean_query}'")
           return []
       
       items = data.get("items", [])
       logging.info(f"üîç google_search: –ù–∞–π–¥–µ–Ω–æ {len(items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
       links = [item.get("link") for item in items if item.get("link")]
       
       # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ URL
       valid_links = [link for link in links if is_valid_url(link)]
       
       elapsed_time = time.time() - start_time
       logging.info(f"‚úÖ google_search: –ù–∞–π–¥–µ–Ω–æ {len(valid_links)} –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
       logs.append(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(valid_links)} —Å—Å—ã–ª–æ–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{clean_query}' –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")
       
       # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
       for i, link in enumerate(valid_links[:3]):
           logging.info(f"URL {i+1}: {link}")
           logs.append(f"URL {i+1}: {link}")
       
       return valid_links
       
   except requests.exceptions.RequestException as e:
       logs.append(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ Google: {str(e)}")
       logger.error(f"–û—à–∏–±–∫–∞ Google API: {str(e)}")
       return []

def is_valid_url(url: str) -> bool:
   """
   –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –¥–æ–ø—É—Å—Ç–∏–º—ã–º.
   
   Args:
       url (str): URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
       
   Returns:
       bool: True, –µ—Å–ª–∏ URL –¥–æ–ø—É—Å—Ç–∏–º, –∏–Ω–∞—á–µ False
   """
   try:
       result = urlparse(url)
       return all([result.scheme, result.netloc])
   except:
       return False

def prioritize_links(links: List[str], query: str) -> List[str]:
    """
    –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ—Ç —Å—Å—ã–ª–∫–∏ –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å—É.
    
    Args:
        links: –°–ø–∏—Å–æ–∫ URL
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        
    Returns:
        –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ URL —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
    """
    # –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã –¥–ª—è –ø—Ä–∞–≤–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    high_quality_domains = [
        "consultant.ru", "garant.ru", "sudact.ru", "pravo.gov.ru", 
        "zakon.ru", "ksrf.ru", "vsrf.ru", "arbitr.ru", "rg.ru", "supcourt.ru",
        "advgazeta.ru", "kodeks.ru", "pravoved.ru", "pravorub.ru", "rostrud.gov.ru"
    ]
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ—Ä—Ç–∞–ª—ã
    info_portals = [
        "tass.ru", "rbc.ru", "kommersant.ru", "vedomosti.ru", "interfax.ru", 
        "pravo.ru", "rapsinews.ru"
    ]
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥–æ–π —Å—Å—ã–ª–∫–∏
    scored_links = []
    
    for url in links:
        score = 0
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.lower()
        path = parsed_url.path.lower()
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ –∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
        bad_domains = ["pinterest", "instagram", "facebook", "twitter", "youtube", "tiktok", 
                       "reddit", "quora", "linkedin", "amazon", "ebay", "avito", "aliexpress"]
        if any(bad_domain in domain for bad_domain in bad_domains):
            continue
        
        # –ü–æ–≤—ã—à–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
        if any(good_domain in domain for good_domain in high_quality_domains):
            score += 15
        
        # –ü–æ–≤—ã—à–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ—Ä—Ç–∞–ª–æ–≤
        if any(portal in domain for portal in info_portals):
            score += 8
        
        # –ü–æ–≤—ã—à–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –ø—Ä–∞–≤–æ–≤—ã—Ö –¥–æ–º–µ–Ω–æ–≤
        if any(term in domain for term in ["garant", "consultant", "pravorub", "pravo", "sudact", "zakon", "advgazeta", "kodeks", "pravoved"]):
            score += 10
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç–∏ URL –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ç–µ–º–∞—Ç–∏–∫–µ
        legal_path_terms = ["sud", "zakon", "kodeks", "pravo", "jurist", "advokat", "urist", "legal", "law"]
        if any(term in path for term in legal_path_terms):
            score += 5
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –ø–æ –Ω–∞–ª–∏—á–∏—é –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –≤ URL
        query_terms = query.lower().split()
        for term in query_terms:
            if len(term) > 3 and term in url.lower():
                score += 3
        
        # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã PDF, DOCX –∏ —Ç.–¥. –¥–ª—è –ø—Ä–∞–≤–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∞–π—Ç–∞—Ö)
        if url.lower().endswith(('.pdf', '.doc', '.docx')):
            score += 5
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±–æ–Ω—É—Å, –µ—Å–ª–∏ PDF –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º —Å–∞–π—Ç–µ
            if any(good_domain in domain for good_domain in high_quality_domains):
                score += 5
        
        # –†—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ —Å–∞–π—Ç—ã –ø–æ–ª—É—á–∞—é—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        if ".ru" in domain or ".—Ä—Ñ" in domain:
            score += 7
        
        # –ü–æ–Ω–∏–∂–∞–µ–º —Ä–µ–π—Ç–∏–Ω–≥ –¥–ª—è —Å–∞–π—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        if any(old_term in domain for old_term in ["narod.ru", "ucoz.ru", "boom.ru", "by.ru"]):
            score -= 5
            
        # –ò–∑–±–µ–≥–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ URL, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏
        if "?" in url and ("search" in url.lower() or "query" in url.lower()):
            score -= 3
        
        scored_links.append((url, score))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ –ø–æ —É–±—ã–≤–∞–Ω–∏—é –æ—Ü–µ–Ω–∫–∏
    scored_links.sort(key=lambda x: x[1], reverse=True)
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ URL, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
    return [url for url, _ in scored_links]


async def search_and_scrape(query: str, logs: list, max_results: int = MAX_SCRAPE_RESULTS, force_refresh: bool = False) -> list:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ Google, –ø–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–µ–±-—Å—Å—ã–ª–æ–∫ –∏ –ø–µ—Ä–µ–¥–∞—ë—Ç –∏—Ö –≤ –º–æ–¥—É–ª—å —Å–∫—Ä–µ–π–ø–µ—Ä–∞ 
    –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü. –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–æ MAX_SCRAPE_RESULTS (–¢–û–ü-5).
    
    Args:
        query (str): –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
        logs (list): —Å–ø–∏—Å–æ–∫ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è.
        max_results (int): –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5).
        force_refresh (bool): —Ñ–ª–∞–≥ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫—ç—à–∞.
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ ScrapedContent, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.
    """
    start_time = time.time()
    logs.append(f"üîç –ù–∞—á–∞–ª–æ –ø–æ–∏—Å–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
    
    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ max_results –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç MAX_SCRAPE_RESULTS (5)
    max_results = min(max_results, MAX_SCRAPE_RESULTS)
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –∏ –ø–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ (–ø–æ–ª—É—á–∞–µ–º –±–æ–ª—å—à–µ, —á—Ç–æ–±—ã –ø–æ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å)
    links = google_search(query, logs, max_results=max_results * 2) 
    
    if not links:
        logs.append("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞")
        return []
    
    try:
        # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏
        prioritized_links = prioritize_links(links, query)
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ (–¢–û–ü-5 –∏–ª–∏ –º–µ–Ω—å—à–µ)
        links_to_scrape = prioritized_links[:max_results]
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞
        logs.append(f"üì• –û—Ç–ø—Ä–∞–≤–∫–∞ –Ω–∞ —Å–∫—Ä–µ–π–ø–∏–Ω–≥ –¢–û–ü-{len(links_to_scrape)} —Å—Å—ã–ª–æ–∫ –∏–∑ {len(links)} –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö")
        logging.info(f"üì• –û—Ç–ø—Ä–∞–≤–∫–∞ –Ω–∞ —Å–∫—Ä–µ–π–ø–∏–Ω–≥ –¢–û–ü-{len(links_to_scrape)} —Å—Å—ã–ª–æ–∫ –∏–∑ {len(links)} –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö")

        if not links_to_scrape:
            logging.warning(f"‚ö†Ô∏è search_and_scrape: –°—Å—ã–ª–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è '{query}'")
            return []
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
        scrape_tasks = []
        for url in links_to_scrape:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –±–∏–Ω–∞—Ä–Ω—ã–º —Ñ–∞–π–ª–æ–º
            is_binary = url.lower().endswith(('.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx'))
            
            if is_binary:
                # –°–æ–∑–¥–∞–µ–º placeholder –¥–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                file_type = url.split('.')[-1].upper()
                result = ScrapedContent(
                    url=url,
                    title=f"{file_type} Document: {url.split('/')[-1]}",
                    text=f"Binary {file_type} file available at: {url}",
                    html="",
                    metadata={"binary_type": file_type.lower(), "scraped": False},
                    content_type=f"application/{file_type.lower()}"
                )
                scrape_tasks.append(asyncio.create_task(asyncio.sleep(0, result)))
                logs.append(f"‚úÖ Created placeholder for binary {file_type}: {url}")
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º scraper –¥–ª—è HTML-–∫–æ–Ω—Ç–µ–Ω—Ç–∞
                scraper = get_scraper()
                task = scraper.scrape_url(url, dynamic=False)
                scrape_tasks.append(task)
        
        # –î–æ–∂–∏–¥–∞–µ–º—Å—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á —Å —Ç–∞–π–º–∞—É—Ç–æ–º –¥–ª—è –∫–∞–∂–¥–æ–π
        # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –º—ã –Ω–µ –±—É–¥–µ–º –∂–¥–∞—Ç—å —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ
        completed_tasks = []
        timeout = 15  # 15 —Å–µ–∫—É–Ω–¥ –º–∞–∫—Å–∏–º—É–º –Ω–∞ –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É
        
        for task in scrape_tasks:
            try:
                result = await asyncio.wait_for(task, timeout=timeout)
                completed_tasks.append(result)
            except asyncio.TimeoutError:
                logs.append(f"‚ö†Ô∏è –°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–Ω—è–ª —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –±—ã–ª –ø—Ä–µ—Ä–≤–∞–Ω")
                logging.warning("–°–∫—Ä–∞–ø–∏–Ω–≥ URL –ø—Ä–µ–≤—ã—Å–∏–ª —Ç–∞–π–º–∞—É—Ç –∏ –±—ã–ª –ø—Ä–µ—Ä–≤–∞–Ω")
            except Exception as e:
                logs.append(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–µ: {str(e)}")
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–µ: {str(e)}")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
        successful_results = []
        max_content_length = 5000  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        
        for result in completed_tasks:
            if result.is_successful():
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞ –≤ –ø—Ä–æ–º—Ç–µ
                if len(result.text) > max_content_length:
                    truncated_text = result.text[:max_content_length] + "... [—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Ä–∞–∑–º–µ—Ä–∞]"
                    result = ScrapedContent(
                        url=result.url,
                        title=result.title,
                        text=truncated_text,
                        html=result.html,
                        metadata=result.metadata,
                        content_type=result.content_type,
                        error=None
                    )
                
                successful_results.append(result)
                logs.append(f"‚úÖ Successfully scraped: {result.url}")
        
        elapsed_time = time.time() - start_time
        logging.info(f"‚úÖ search_and_scrape: –ü–æ–ª—É—á–µ–Ω–æ {len(successful_results)} —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ {len(links_to_scrape)} —Å—Å—ã–ª–æ–∫ –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
        logs.append(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(successful_results)} —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ {len(links_to_scrape)} —Å—Å—ã–ª–æ–∫ –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        return successful_results
        
    except Exception as e:
        logs.append(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–µ: {str(e)}")
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–µ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}': {str(e)}")
        return []


async def run_multiple_searches(query: str, logs: list, force_refresh: bool = False) -> Dict[str, List]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø–æ–∏—Å–∫ –≤–º–µ—Å—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–æ MAX_SCRAPE_RESULTS (–¢–û–ü-5).
    """
    logs.append(f"üîÑ –ó–∞–ø—É—Å–∫ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø–æ–∏—Å–∫ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–æ MAX_SCRAPE_RESULTS (5) —Å—Å—ã–ª–æ–∫
    try:
        general_results = await search_and_scrape(query, logs, max_results=MAX_SCRAPE_RESULTS, force_refresh=force_refresh)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å–ª–æ–≤–∞—Ä—å 
        results = {
            "general": general_results,
            # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏
            "legal": [],
            "recent": []
        }
        
        total_results = len(general_results)
        logs.append(f"üìä –ü–æ–ª—É—á–µ–Ω–æ –≤—Å–µ–≥–æ {total_results} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ –ø–æ–∏—Å–∫–∞ (–º–∞–∫—Å–∏–º—É–º {MAX_SCRAPE_RESULTS})")
        
        return results
    
    except Exception as e:
        logs.append(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø–æ–∏—Å–∫–∞: {str(e)}")
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø–æ–∏—Å–∫–∞: {str(e)}")
        return {"legal": [], "recent": [], "general": []}