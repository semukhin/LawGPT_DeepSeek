"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ.
–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google Custom Search API –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.
"""
import requests
import urllib3
import logging
import asyncio
import time
from typing import List, Dict, Any, Optional, Union, Tuple
from urllib.parse import urlparse, urljoin
import os
import sys
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import aiohttp
from bs4 import Comment
from app.utils import ensure_correct_encoding, sanitize_search_results, validate_messages, validate_context
from app.search_result import SearchResult
import ssl
import random
from app.utils.text_utils import extract_keywords_ru

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ third_party –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞ shandu
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
THIRD_PARTY_DIR = os.path.join(BASE_DIR, "third_party")
if THIRD_PARTY_DIR not in sys.path:
   sys.path.insert(0, THIRD_PARTY_DIR)

from third_party.shandu.scraper import ScrapedContent
from app.services.web_scraper import WebScraper

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# –ö–ª—é—á–∏ Google Custom Search
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY", "")
GOOGLE_CX = os.environ.get("GOOGLE_CX", "")

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Å—ã–ª–æ–∫
MAX_SEARCH_RESULTS = 10  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞
MAX_SCRAPE_RESULTS = 10   # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞ (–¢–û–ü-7)
MAX_CONTENT_LENGTH = 30000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞

# –í –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞
logging.info(f"Google API Key –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {'–î–∞' if len(GOOGLE_API_KEY) > 0 else '–ù–µ—Ç'}")
logging.info(f"Google CX –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {'–î–∞' if len(GOOGLE_CX) > 0 else '–ù–µ—Ç'}")

# –°–æ–∑–¥–∞–µ–º –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä WebScraper
_scraper = None

def get_scraper() -> WebScraper:
   """
   –ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä WebScraper.
   
   Returns:
       WebScraper: –≠–∫–∑–µ–º–ø–ª—è—Ä —Å–∫—Ä–∞–ø–µ—Ä–∞
   """
   global _scraper
   if _scraper is None:
       _scraper = WebScraper(
           timeout=20,
           max_retries=2,
           max_concurrent=8
       )
       logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω WebScraper")
   return _scraper

def google_search(query: str, logs: list, max_results: int = MAX_SEARCH_RESULTS) -> list:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É —á–µ—Ä–µ–∑ Google Custom Search API –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤–µ–±-—Å—Å—ã–ª–æ–∫.
    
    Args:
        query (str): –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
        logs (list): —Å–ø–∏—Å–æ–∫ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ö–æ–¥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.
        max_results (int): –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ URL, –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–æ –∑–∞–ø—Ä–æ—Å—É.
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∫–ª—é—á–µ–Ω –ª–∏ –ø–æ–∏—Å–∫ –≤ Google –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö
    google_search_enabled = os.environ.get("GOOGLE_SEARCH_ENABLED", "True").lower() == "true"
    if not google_search_enabled:
        logging.info("üîç google_search: –ü–æ–∏—Å–∫ –≤ Google –æ—Ç–∫–ª—é—á–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö")
        logs.append("‚ö†Ô∏è –ü–æ–∏—Å–∫ –≤ Google –æ—Ç–∫–ª—é—á–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö")
        return []

    # --- –ù–æ–≤—ã–π –±–ª–æ–∫: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ ---
    orig_query = query.strip()
    if len(orig_query) > 200:
        keywords_query = extract_keywords_ru(orig_query)
        logs.append(f"üîë –ò–∑–≤–ª–µ—á–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞: {keywords_query}")
        logging.info(f"üîë –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {orig_query}")
        logging.info(f"üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞: {keywords_query}")
        clean_query = keywords_query
    else:
        clean_query = ensure_correct_encoding(orig_query)
        logging.info(f"üîç google_search: –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ –¥–ª—è '{clean_query}'")
    
    # –û—á–∏—â–∞–µ–º –∫–ª—é—á–∏ API –æ—Ç –∫–∞–≤—ã—á–µ–∫
    api_key = GOOGLE_API_KEY.replace('"', '') if GOOGLE_API_KEY else ""
    cx_id = GOOGLE_CX.replace('"', '') if GOOGLE_CX else ""
    
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": api_key,
        "cx": cx_id,
        "q": clean_query,
        "num": min(max_results, 10),  # Google API –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        "safe": "active",
        "hl": "ru",  # –Ø–∑—ã–∫ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
        "gl": "ru",  # –ì–µ–æ–ª–æ–∫–∞—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞
        "lr": "lang_ru"  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ —è–∑—ã–∫—É
    }
    
    logs.append(f"üîç –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ Google Custom Search API –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{clean_query}'")
    start_time = time.time()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–ª—é—á–µ–π
    if not GOOGLE_API_KEY or not GOOGLE_CX:
        logging.warning("‚ùå google_search: API –∫–ª—é—á–∏ Google –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã!")
        logs.append("‚ùå API –∫–ª—é—á–∏ Google –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã!")
        return []

    try:
        logging.info(f"üîç google_search: –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ API")
        response = requests.get(url, params=params, timeout=15)
        status_code = response.status_code
        logging.info(f"üîç google_search: –°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞: {status_code}")
        
        if status_code == 403:
            error_msg = "–î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω (403). –í–æ–∑–º–æ–∂–Ω–æ, API –∫–ª—é—á –Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∏–ª–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∞ –∫–≤–æ—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤."
            logging.error(f"‚ùå google_search: {error_msg}")
            logs.append(f"‚ùå –û—à–∏–±–∫–∞ Google API: {error_msg}")
            try:
                error_details = response.json()
                if 'error' in error_details:
                    logging.error(f"–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏ Google API: {error_details['error']}")
                    logs.append(f"–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {error_details['error'].get('message', '–ù–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏')}")
            except:
                pass
            return []
        
        response.raise_for_status()
        data = response.json()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if "items" not in data:
            logging.warning(f"‚ö†Ô∏è google_search: API –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            logs.append(f"‚ö†Ô∏è Google –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{clean_query}'")
            return []
        
        items = data.get("items", [])
        logging.info(f"üîç google_search: –ù–∞–π–¥–µ–Ω–æ {len(items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        links = [item.get("link") for item in items if item.get("link")]
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ URL
        valid_links = [link for link in links if is_valid_url(link)]
        
        elapsed_time = time.time() - start_time
        logging.info(f"‚úÖ google_search: –ù–∞–π–¥–µ–Ω–æ {len(valid_links)} –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
        logs.append(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(valid_links)} —Å—Å—ã–ª–æ–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{clean_query}' –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        for i, link in enumerate(valid_links[:10]):
            logging.info(f"URL {i+1}: {link}")
            logs.append(f"URL {i+1}: {link}")
        
        return valid_links
        
    except requests.exceptions.RequestException as e:
        error_msg = str(e)
        logs.append(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ Google: {error_msg}")
        logger.error(f"–û—à–∏–±–∫–∞ Google API: {error_msg}")
        return []

def is_valid_url(url: str) -> bool:
   """
   –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –¥–æ–ø—É—Å—Ç–∏–º—ã–º.
   
   Args:
       url (str): URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
       
   Returns:
       bool: True, –µ—Å–ª–∏ URL –¥–æ–ø—É—Å—Ç–∏–º, –∏–Ω–∞—á–µ False
   """
   try:
       result = urlparse(url)
       return all([result.scheme, result.netloc])
   except:
       return False

def clean_soup(soup: BeautifulSoup) -> BeautifulSoup:
    """
    –û—á–∏—â–∞–µ—Ç BeautifulSoup –æ–±—ä–µ–∫—Ç –æ—Ç –Ω–µ–Ω—É–∂–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤.
    
    Args:
        soup: BeautifulSoup –æ–±—ä–µ–∫—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        
    Returns:
        BeautifulSoup: –û—á–∏—â–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç
    """
    # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
    for script in soup(["script", "style", "meta", "link", "noscript", "iframe", "header", "footer", "nav"]):
        script.decompose()

    # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
    for comment in soup.findAll(text=lambda text: isinstance(text, Comment)):
        comment.extract()

    return soup

def get_text_from_soup(soup: BeautifulSoup) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ BeautifulSoup –æ–±—ä–µ–∫—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.
    
    Args:
        soup: BeautifulSoup –æ–±—ä–µ–∫—Ç
        
    Returns:
        str: –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    # –ù–∞—Ö–æ–¥–∏–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
    main_content = None
    
    # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω—ã–º –∫–ª–∞—Å—Å–∞–º –∏ ID
    content_identifiers = [
        {"class_": ["content", "article", "post", "entry", "main", "text"]},
        {"id": ["content", "article", "post", "entry", "main", "text"]}
    ]
    
    for identifier in content_identifiers:
        main_content = soup.find(**identifier)
        if main_content:
            break
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º/ID, –±–µ—Ä–µ–º –≤–µ—Å—å body
    if not main_content:
        main_content = soup.body if soup.body else soup
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    text_parts = []
    
    for element in main_content.descendants:
        if element.name in ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
            text = element.get_text(strip=True)
            if text:
                text_parts.append(text + '\n')
        elif element.name == 'br':
            text_parts.append('\n')
        elif element.name in ['ul', 'ol']:
            for li in element.find_all('li', recursive=False):
                text = li.get_text(strip=True)
                if text:
                    text_parts.append(f"‚Ä¢ {text}\n")
    
    return ''.join(text_parts)

def extract_title(soup: BeautifulSoup) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    
    Args:
        soup: BeautifulSoup –æ–±—ä–µ–∫—Ç
        
    Returns:
        str: –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    """
    # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º h1
    h1 = soup.find('h1')
    if h1:
        title = h1.get_text(strip=True)
        if title:
            return title
    
    # –ï—Å–ª–∏ h1 –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç–æ–π, –∏—â–µ–º title
    title_tag = soup.find('title')
    if title_tag:
        return title_tag.get_text(strip=True)
    
    return "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"

def get_relevant_images(soup: BeautifulSoup, base_url: str) -> List[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç URL —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
    
    Args:
        soup: BeautifulSoup –æ–±—ä–µ–∫—Ç
        base_url: –ë–∞–∑–æ–≤—ã–π URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    """
    images = []
    for img in soup.find_all('img'):
        src = img.get('src')
        if src:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
            full_url = urljoin(base_url, src)
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∫–ª–∞–º–Ω—ã–µ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if not any(x in full_url.lower() for x in ['avatar', 'logo', 'banner', 'ad', 'icon']):
                images.append(full_url)
    return images

async def scrape_with_beautifulsoup(url: str) -> Optional[ScrapedContent]:
    """
    –°–∫—Ä–µ–π–ø–∏—Ç –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –ø–æ–º–æ—â—å—é BeautifulSoup.
    
    Args:
        url (str): URL –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞
        
    Returns:
        Optional[ScrapedContent]: –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        }
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è e.law.ru
        if 'e.law.ru' in url:
            headers.update({
                'Referer': 'https://e.law.ru/',
                'Origin': 'https://e.law.ru',
                'Host': 'e.law.ru'
            })
            
        async with aiohttp.ClientSession(headers=headers) as session:
            async with session.get(url, ssl=False, timeout=30) as response:
                if response.status != 200:
                    logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–µ {url}: —Å—Ç–∞—Ç—É—Å {response.status}")
                    return None
                    
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏–ª–∏ meta-—Ç–µ–≥–æ–≤
                content_type = response.headers.get('Content-Type', '')
                charset = None
                if 'charset=' in content_type:
                    charset = content_type.split('charset=')[-1].strip()
                
                content = await response.read()
                
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
                for encoding in [charset, 'utf-8', 'windows-1251', 'cp1251', 'latin1']:
                    if encoding:
                        try:
                            html_content = content.decode(encoding)
                            break
                        except (UnicodeDecodeError, LookupError):
                            continue
                else:
                    # –ï—Å–ª–∏ –Ω–∏ –æ–¥–Ω–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∞ –Ω–µ –ø–æ–¥–æ—à–ª–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º utf-8 —Å –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—à–∏–±–æ–∫
                    html_content = content.decode('utf-8', errors='ignore')
                
                # –î–ª—è e.law.ru –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è JavaScript
                if 'e.law.ru' in url and 'window.__INITIAL_STATE__' in html_content:
                    try:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ JavaScript
                        start_idx = html_content.find('window.__INITIAL_STATE__ = ') + len('window.__INITIAL_STATE__ = ')
                        end_idx = html_content.find('</script>', start_idx)
                        json_data = html_content[start_idx:end_idx].strip().rstrip(';')
                        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –∏ –∏–∑–≤–ª–µ—á—å –Ω—É–∂–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
                        # ...
                    except Exception as e:
                        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JavaScript –Ω–∞ e.law.ru: {e}")
                
                soup = BeautifulSoup(html_content, 'html.parser')
                soup = clean_soup(soup)
                
                title = extract_title(soup)
                text = get_text_from_soup(soup)
                
                if not text:
                    logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ {url}")
                    return None
                    
                return ScrapedContent(
                    url=url,
                    title=title,
                    text=text,
                    html=html_content,
                    metadata={},
                    content_type="text/html"
                )
                
    except asyncio.TimeoutError:
        logging.error(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–µ {url}")
        return None
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–µ {url} —Å BeautifulSoup: {e}, message='{str(e)}', url='{url}'")
        return None

async def filter_urls(urls: List[str], excluded_domains: List[str] = None) -> List[str]:
    """
    –§–∏–ª—å—Ç—Ä—É–µ—Ç URL –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–∏—Å–∫–∞ –∏—Å–∫–ª—é—á–∞–µ–º—ã—Ö –¥–æ–º–µ–Ω–æ–≤.
    
    Args:
        urls (List[str]): –°–ø–∏—Å–æ–∫ URL –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        excluded_domains (List[str], optional): –°–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        
    Returns:
        List[str]: –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ URL
    """
    if excluded_domains is None:
        excluded_domains = [
            'facebook.com', 'twitter.com', 'instagram.com', 'youtube.com',
            'linkedin.com', 'pinterest.com', 'tiktok.com', 'reddit.com',
            'vk.com', 'ok.ru', 'mail.ru', 'yandex.ru'
        ]
    
    filtered_urls = []
    for url in urls:
        if not any(excluded in url.lower() for excluded in excluded_domains):
            filtered_urls.append(url)
    return filtered_urls

async def extract_main_content(html_content: str, url: str) -> Tuple[str, List[str]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ HTML —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.
    
    Args:
        html_content (str): HTML –∫–æ–Ω—Ç–µ–Ω—Ç
        url (str): URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
    Returns:
        Tuple[str, List[str]]: (–æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç, —Å–ø–∏—Å–æ–∫ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
    """
    try:
        soup = BeautifulSoup(html_content, "lxml")
        soup = clean_soup(soup)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        main_content = None
        content_identifiers = [
            {"class_": ["content", "article", "post", "entry", "main", "text"]},
            {"id": ["content", "article", "post", "entry", "main", "text"]}
        ]
        
        for identifier in content_identifiers:
            main_content = soup.find(**identifier)
            if main_content:
                break
        
        if not main_content:
            main_content = soup.body if soup.body else soup
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        text_parts = []
        for element in main_content.descendants:
            if element.name in ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                text = element.get_text(strip=True)
                if text:
                    text_parts.append(text + '\n')
            elif element.name == 'br':
                text_parts.append('\n')
            elif element.name in ['ul', 'ol']:
                for li in element.find_all('li', recursive=False):
                    text = li.get_text(strip=True)
                    if text:
                        text_parts.append(f"‚Ä¢ {text}\n")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        images = get_relevant_images(soup, url)
        
        return ''.join(text_parts), images
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {str(e)}")
        return "", []

async def process_scraped_data(scraped_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞.
    
    Args:
        scraped_data: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞
        
    Returns:
        List[Dict[str, Any]]: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    processed_data = []
    
    for item in scraped_data:
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É —Ç–µ–∫—Å—Ç–∞
            if isinstance(item.get('text'), str):
                item['text'] = ensure_correct_encoding(item['text'])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
            if isinstance(item.get('title'), str):
                item['title'] = ensure_correct_encoding(item['title'])
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É URL
            if isinstance(item.get('url'), str):
                item['url'] = ensure_correct_encoding(item['url'])
                
            processed_data.append(item)
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞: {str(e)}")
            continue
            
    return processed_data

async def search_and_scrape(query: str, logs: list, max_results: int = MAX_SCRAPE_RESULTS, force_refresh: bool = False) -> list:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –∏ —Å–∫—Ä–µ–π–ø–∏–Ω–≥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É.
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        logs: –°–ø–∏—Å–æ–∫ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        force_refresh: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    session = None
    try:
        # –°–æ–∑–¥–∞–µ–º –æ–¥–Ω—É —Å–µ—Å—Å–∏—é –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        session = aiohttp.ClientSession()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞
        query = ensure_correct_encoding(query)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ —á–µ—Ä–µ–∑ Google API
        links = google_search(query, logs, max_results)
        if not links:
            return []
            
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Å—ã–ª–∫–∏
        filtered_links = await filter_urls(links)
        if not filtered_links:
            return []
            
        # –°–∫—Ä–µ–π–ø–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç
        scraped_results = []
        for url in filtered_links[:max_results]:
            try:
                content = await scrape_with_beautifulsoup(url)
                if content and content.text.strip():
                    scraped_results.append({
                        'url': url,
                        'text': content.text,
                        'title': content.title
                    })
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–µ {url}: {str(e)}")
                continue
                
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        processed_results = await process_scraped_data(scraped_results)
        
        return processed_results
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –≤ search_and_scrape: {str(e)}")
        return []
    finally:
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–µ—Å—Å–∏—é –≤ –±–ª–æ–∫–µ finally
        if session:
            await session.close()


async def run_multiple_searches(query: str, logs: list, force_refresh: bool = False) -> Dict[str, List]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø–æ–∏—Å–∫ –≤–º–µ—Å—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–æ MAX_SCRAPE_RESULTS (–¢–û–ü-7).
    """
    logs.append(f"üîÑ –ó–∞–ø—É—Å–∫ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø–æ–∏—Å–∫ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–æ MAX_SCRAPE_RESULTS (7) —Å—Å—ã–ª–æ–∫
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ —á–µ—Ä–µ–∑ Google API
        links = google_search(query, logs, max_results=MAX_SCRAPE_RESULTS)
        if not links:
            return {"general": [], "legal": [], "recent": []}
            
        # –°–∫—Ä–µ–π–ø–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç
        scraped_results = []
        for url in links[:MAX_SCRAPE_RESULTS]:
            try:
                content = await scrape_with_beautifulsoup(url)
                if content and content.text.strip():
                    scraped_results.append({
                        'url': url,
                        'title': content.title,
                        'text': content.text[:2000],  # –ë–µ—Ä–µ–º –¥–æ 2000 —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                        'source': 'google'
                    })
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–µ {url}: {str(e)}")
                continue
                
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        processed_results = await process_scraped_data(scraped_results)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å–ª–æ–≤–∞—Ä—å 
        results = {
            "general": processed_results,
            # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏
            "legal": [],
            "recent": []
        }
        
        total_results = len(processed_results)
        logs.append(f"üìä –ü–æ–ª—É—á–µ–Ω–æ –≤—Å–µ–≥–æ {total_results} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ –ø–æ–∏—Å–∫–∞ (–º–∞–∫—Å–∏–º—É–º {MAX_SCRAPE_RESULTS})")
        
        return results
    
    except Exception as e:
        logs.append(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø–æ–∏—Å–∫–∞: {str(e)}")
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø–æ–∏—Å–∫–∞: {str(e)}")
        return {"legal": [], "recent": [], "general": []}

async def search_internet(query: str, max_results: int = 5) -> List[SearchResult]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ —Å –ø–æ–º–æ—â—å—é Google.
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        List[SearchResult]: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ —á–µ—Ä–µ–∑ Google API
        links = google_search(query, [], max_results)
        if not links:
            return []
            
        # –°–∫—Ä–µ–π–ø–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç
        scraper = get_scraper()
        results = []
        
        for url in links[:max_results]:
            try:
                content = await scraper.scrape_url(url)
                if content and content.is_successful():
                    result = SearchResult(
                        url=url,
                        title=content.title,
                        snippet=content.text[:200] + "...",
                        source="google"
                    )
                    results.append(result)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–µ {url}: {e}")
                continue
                
        return results
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {e}")
        return []

async def _async_google_search(query: str, max_results: int = 5) -> List[Dict[str, Any]]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google Custom Search API.
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∫–ª—é—á–∏ API –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
        api_key = os.getenv("GOOGLE_API_KEY")
        cx = os.getenv("GOOGLE_CX")
        
        if not api_key or not cx:
            logger.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–ª—é—á–∏ Google API")
            return []
            
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            "key": api_key,
            "cx": cx,
            "q": query,
            "num": max_results
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params) as response:
                if response.status != 200:
                    logger.error(f"–û—à–∏–±–∫–∞ Google API: {response.status}")
                    return []
                    
                data = await response.json()
                if "items" not in data:
                    return []
                    
                return [{
                    "url": item.get("link", ""),
                    "title": item.get("title", ""),
                    "snippet": item.get("snippet", ""),
                    "source": "google",
                    "date": item.get("pagemap", {}).get("metatags", [{}])[0].get("article:published_time"),
                    "metadata": item.get("pagemap", {})
                } for item in data["items"]]
                
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ Google: {e}")
        return []