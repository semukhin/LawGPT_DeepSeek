"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ.
–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google Custom Search API –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.
"""
import requests
import urllib3
import logging
import asyncio
import time
from typing import List, Dict, Any, Optional, Union
from urllib.parse import urlparse
import os
import sys
from dotenv import load_dotenv

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ third_party –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞ shandu
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
THIRD_PARTY_DIR = os.path.join(BASE_DIR, "third_party")
if THIRD_PARTY_DIR not in sys.path:
    sys.path.insert(0, THIRD_PARTY_DIR)

from app.utils import get_url_content
from third_party.shandu.scraper import WebScraper, ScrapedContent

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# –ö–ª—é—á–∏ Google Custom Search
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY", "")
GOOGLE_CX = os.environ.get("GOOGLE_CX", "")

# –°–æ–∑–¥–∞–µ–º –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä WebScraper
_scraper = None


def get_scraper() -> WebScraper:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä WebScraper –∏–∑ Shandu.
    
    Returns:
        WebScraper: –≠–∫–∑–µ–º–ø–ª—è—Ä —Å–∫—Ä–∞–ø–µ—Ä–∞ Shandu
    """
    global _scraper
    if _scraper is None:
        # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä WebScraper –∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –µ–≥–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        _scraper = WebScraper(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            respect_robots=False)
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω WebScraper –∏–∑ Shandu")
    return _scraper


def google_search(query: str, logs: list, max_results: int = 10) -> list:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É —á–µ—Ä–µ–∑ Google Custom Search API –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤–µ–±-—Å—Å—ã–ª–æ–∫.
    
    Args:
        query (str): –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
        logs (list): —Å–ø–∏—Å–æ–∫ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ö–æ–¥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.
        max_results (int): –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ URL, –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–æ –∑–∞–ø—Ä–æ—Å—É.
    """
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": GOOGLE_API_KEY,
        "cx": GOOGLE_CX,
        "q": query,
        "num": min(max_results, 5),  # Google API –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –¥–æ 5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        "safe": "active"
    }
    
    logs.append(f"üîç –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ Google Custom Search API –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
    start_time = time.time()
    
    try:
        response = requests.get(url, params=params, timeout=15)
        response.raise_for_status()
        data = response.json()
        
        if "items" not in data:
            logs.append(f"‚ö†Ô∏è Google –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}'")
            return []
        
        items = data.get("items", [])
        links = [item.get("link") for item in items if item.get("link")]
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ URL
        valid_links = [link for link in links if is_valid_url(link)]
        
        elapsed_time = time.time() - start_time
        logs.append(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(valid_links)} —Å—Å—ã–ª–æ–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}' –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        for i, link in enumerate(valid_links[:3]):
            logs.append(f"URL {i+1}: {link}")
        
        return valid_links
        
    except requests.exceptions.RequestException as e:
        logs.append(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ Google: {str(e)}")
        logger.error(f"–û—à–∏–±–∫–∞ Google API: {str(e)}")
        return []

def is_valid_url(url: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –¥–æ–ø—É—Å—Ç–∏–º—ã–º.
    
    Args:
        url (str): URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        
    Returns:
        bool: True, –µ—Å–ª–∏ URL –¥–æ–ø—É—Å—Ç–∏–º, –∏–Ω–∞—á–µ False
    """
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

def prioritize_links(links: List[str], query: str) -> List[str]:
    """
    –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ—Ç —Å—Å—ã–ª–∫–∏ –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å—É.
    
    Args:
        links: –°–ø–∏—Å–æ–∫ URL
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        
    Returns:
        –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ URL —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
    """
    # –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã –¥–ª—è –ø—Ä–∞–≤–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    high_quality_domains = [
        "consultant.ru", "garant.ru", "sudact.ru", "pravo.gov.ru", 
        "zakon.ru", "ksrf.ru", "vsrf.ru", "arbitr.ru"
    ]
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥–æ–π —Å—Å—ã–ª–∫–∏
    scored_links = []
    
    for url in links:
        score = 0
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.lower()
        
        # –ü–æ–≤—ã—à–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
        if any(good_domain in domain for good_domain in high_quality_domains):
            score += 10
        
        # –ü–æ–≤—ã—à–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –ø—Ä–∞–≤–æ–≤—ã—Ö –¥–æ–º–µ–Ω–æ–≤
        if any(term in domain for term in ["garant", "consultant", "pravorub", "pravo", "sudact", "zakon"]):
            score += 5
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –ø–æ –Ω–∞–ª–∏—á–∏—é –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –≤ URL
        query_terms = query.lower().split()
        for term in query_terms:
            if len(term) > 3 and term in url.lower():
                score += 2
        
        # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã PDF, DOCX –∏ —Ç.–¥. –¥–ª—è –ø—Ä–∞–≤–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        if url.lower().endswith(('.pdf', '.doc', '.docx')):
            score += 3
        
        # –†—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ —Å–∞–π—Ç—ã –ø–æ–ª—É—á–∞—é—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        if ".ru" in domain or "—Ä—Ñ" in domain:
            score += 5
        
        scored_links.append((url, score))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ –ø–æ —É–±—ã–≤–∞–Ω–∏—é –æ—Ü–µ–Ω–∫–∏
    scored_links.sort(key=lambda x: x[1], reverse=True)
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ URL, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
    return [url for url, _ in scored_links]


async def search_and_scrape(query: str, logs: list, max_results: int = 5, force_refresh: bool = False) -> list:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ Google, –ø–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–µ–±-—Å—Å—ã–ª–æ–∫ –∏ –ø–µ—Ä–µ–¥–∞—ë—Ç –∏—Ö –≤ –º–æ–¥—É–ª—å —Å–∫—Ä–µ–π–ø–µ—Ä–∞ 
    –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü.
    
    Args:
        query (str): –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
        logs (list): —Å–ø–∏—Å–æ–∫ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è.
        max_results (int): –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        force_refresh (bool): —Ñ–ª–∞–≥ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫—ç—à–∞.
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ ScrapedContent, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.
    """
    start_time = time.time()
    logs.append(f"üîç –ù–∞—á–∞–ª–æ –ø–æ–∏—Å–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –∏ –ø–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏
    links = google_search(query, logs, max_results=max_results*2)  # –ü–æ–ª—É—á–∞–µ–º –±–æ–ª—å—à–µ —Å—Å—ã–ª–æ–∫, —á–µ–º –Ω—É–∂–Ω–æ
    
    if not links:
        logs.append("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞")
        return []
    
    try:
        # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏
        prioritized_links = prioritize_links(links, query)
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫
        links_to_scrape = prioritized_links[:max_results]
        logs.append(f"üì• –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∏–∑ {len(links_to_scrape)} —Å—Å—ã–ª–æ–∫...")
        
        successful_results = []
        for url in links_to_scrape:
            try:
                # Check if the URL points to a binary file
                is_binary = url.lower().endswith(('.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx'))
                
                if is_binary:
                    # Handle binary files differently
                    logs.append(f"üìÑ Detected binary file: {url}")
                    
                    # For PDFs, create a placeholder ScrapedContent with metadata
                    if url.lower().endswith('.pdf'):
                        # Create placeholder with file info but don't try to decode content
                        result = ScrapedContent(
                            url=url,
                            title=f"PDF Document: {url.split('/')[-1]}",
                            text=f"Binary PDF file available at: {url}",
                            html="",
                            metadata={"binary_type": "pdf", "scraped": False},
                            content_type="application/pdf"
                        )
                        successful_results.append(result)
                        logs.append(f"‚úÖ Created placeholder for binary PDF: {url}")
                    else:
                        # For other binary types, create appropriate placeholders
                        file_type = url.split('.')[-1].upper()
                        result = ScrapedContent(
                            url=url,
                            title=f"{file_type} Document: {url.split('/')[-1]}",
                            text=f"Binary {file_type} file available at: {url}",
                            html="",
                            metadata={"binary_type": file_type.lower(), "scraped": False},
                            content_type=f"application/{file_type.lower()}"
                        )
                        successful_results.append(result)
                        logs.append(f"‚úÖ Created placeholder for binary {file_type}: {url}")
                else:
                    # Use regular scraping for non-binary files
                    # Use existing scraper for HTML content
                    scraper = get_scraper()
                    result = await scraper.scrape_url(url, dynamic=False)
                    
                    if result.is_successful():
                        successful_results.append(result)
                        logs.append(f"‚úÖ Successfully scraped: {url}")
                    else:
                        logs.append(f"‚ùå Failed to scrape: {url} - {result.error}")
            except Exception as e:
                logs.append(f"‚ùå Error processing URL {url}: {str(e)}")
                logger.error(f"Error processing URL {url}: {str(e)}")
        
        return successful_results
        
    except Exception as e:
        logs.append(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–µ: {str(e)}")
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–µ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}': {str(e)}")
        return []



async def run_multiple_searches(query: str, logs: list, force_refresh: bool = False) -> Dict[str, List]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
    
    Args:
        query (str): –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        logs (list): –°–ø–∏—Å–æ–∫ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        force_refresh (bool): –§–ª–∞–≥ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫—ç—à–∞
        
    Returns:
        Dict[str, list]: –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞
    """
    logs.append(f"üîÑ –ó–∞–ø—É—Å–∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    legal_query = f"{query} –∑–∞–∫–æ–Ω —Å—É–¥–µ–±–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞"
    recent_query = f"{query} –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è"
    general_query = query
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –ø–æ–∏—Å–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    try:
        legal_search_task = asyncio.create_task(
            search_and_scrape(legal_query, logs, max_results=3, force_refresh=force_refresh)
        )
        
        recent_search_task = asyncio.create_task(
            search_and_scrape(recent_query, logs, max_results=2, force_refresh=force_refresh)
        )
        
        general_search_task = asyncio.create_task(
            search_and_scrape(general_query, logs, max_results=2, force_refresh=force_refresh)
        )
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á —Å –æ–±—â–∏–º —Ç–∞–π–º–∞—É—Ç–æ–º
        tasks = [legal_search_task, recent_search_task, general_search_task]
        
        try:
            results_list = await asyncio.wait_for(asyncio.gather(*tasks), timeout=60)
            legal_results, recent_results, general_results = results_list
        except asyncio.TimeoutError:
            logs.append("‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –æ–±—â–∏–π —Ç–∞–π–º–∞—É—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ —É—Å–ø–µ–ª–∏ –∑–∞–≤–µ—Ä—à–∏—Ç—å—Å—è
            legal_results = []
            recent_results = []
            general_results = []
            
            if legal_search_task.done():
                legal_results = legal_search_task.result()
            else:
                legal_search_task.cancel()
                
            if recent_search_task.done():
                recent_results = recent_search_task.result()
            else:
                recent_search_task.cancel()
                
            if general_search_task.done():
                general_results = general_search_task.result()
            else:
                general_search_task.cancel()
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å–ª–æ–≤–∞—Ä—å
        results = {
            "legal": legal_results,
            "recent": recent_results,
            "general": general_results
        }
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        total_results = sum(len(results[key]) for key in results)
        logs.append(f"üìä –ü–æ–ª—É—á–µ–Ω–æ –≤—Å–µ–≥–æ {total_results} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞")
        
        return results
    
    except Exception as e:
        logs.append(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {str(e)}")
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {str(e)}")
        return {"legal": [], "recent": [], "general": []}


# –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python web_search.py '–ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å'")
        sys.exit(1)
    
    query = sys.argv[1]
    logs = []
    
    print(f"–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
    results = asyncio.run(search_and_scrape(query, logs))
    
    print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
    
    for i, res in enumerate(results, 1):
        if res.is_successful():
            print(f"\n[{i}] URL: {res.url}")
            print(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {res.title}")
            print(f"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤):\n{res.text[:200]}...\n{'-'*80}")
        else:
            print(f"\n[{i}] –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å {res.url}: {res.error}")
    
    print("\n–õ–æ–≥–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:")
    for log in logs:
        print(log)