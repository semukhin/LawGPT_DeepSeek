"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ.
–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google Custom Search API –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.
"""
import requests
import urllib3
import logging
import asyncio
import time
from typing import List, Dict, Any, Optional, Union
from urllib.parse import urlparse
import os
import sys
from dotenv import load_dotenv

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ third_party –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞ shandu
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
THIRD_PARTY_DIR = os.path.join(BASE_DIR, "third_party")
if THIRD_PARTY_DIR not in sys.path:
   sys.path.insert(0, THIRD_PARTY_DIR)

from app.utils import get_url_content
from third_party.shandu.scraper import WebScraper, ScrapedContent

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# –ö–ª—é—á–∏ Google Custom Search
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY", "")
GOOGLE_CX = os.environ.get("GOOGLE_CX", "")


# –í –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞
logging.info(f"Google API Key –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {'–î–∞' if len(GOOGLE_API_KEY) > 0 else '–ù–µ—Ç'}")
logging.info(f"Google CX –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {'–î–∞' if len(GOOGLE_CX) > 0 else '–ù–µ—Ç'}")

# –°–æ–∑–¥–∞–µ–º –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä WebScraper
_scraper = None


def get_scraper() -> WebScraper:
   """
   –ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä WebScraper –∏–∑ Shandu.
   
   Returns:
       WebScraper: –≠–∫–∑–µ–º–ø–ª—è—Ä —Å–∫—Ä–∞–ø–µ—Ä–∞ Shandu
   """
   global _scraper
   if _scraper is None:
       # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä WebScraper –∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –µ–≥–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
       _scraper = WebScraper(
           user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
           respect_robots=False)
       logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω WebScraper –∏–∑ Shandu")
   return _scraper


def google_search(query: str, logs: list, max_results: int = 10) -> list:
   """
   –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É —á–µ—Ä–µ–∑ Google Custom Search API –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤–µ–±-—Å—Å—ã–ª–æ–∫.
   
   Args:
       query (str): –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
       logs (list): —Å–ø–∏—Å–æ–∫ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ö–æ–¥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.
       max_results (int): –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
       
   Returns:
       list: –°–ø–∏—Å–æ–∫ URL, –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–æ –∑–∞–ø—Ä–æ—Å—É.
   """
   # –£–±–∏—Ä–∞–µ–º –ª—é–±—ã–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∫ –∑–∞–ø—Ä–æ—Å—É —Ç–∏–ø–∞ "–∑–∞–∫–æ–Ω —Å—É–¥–µ–±–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞" –∏–ª–∏ "–ø–æ—Å–ª–µ–¥–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è"
   clean_query = query.replace(" –∑–∞–∫–æ–Ω —Å—É–¥–µ–±–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞", "").replace(" –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è", "")
   
   url = "https://www.googleapis.com/customsearch/v1"
   params = {
       "key": GOOGLE_API_KEY,
       "cx": GOOGLE_CX,
       "q": clean_query,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
       "num": min(max_results, 5),  # Google API –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –¥–æ 5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
       "safe": "active"
   }
   
   logs.append(f"üîç –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ Google Custom Search API –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{clean_query}'")
   logging.info(f"üîç google_search: –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ –¥–ª—è '{clean_query}'")
   start_time = time.time()

   # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–ª—é—á–µ–π
   if not GOOGLE_API_KEY or not GOOGLE_CX:
       logging.warning("‚ùå google_search: API –∫–ª—é—á–∏ Google –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã!")
       logs.append("‚ùå API –∫–ª—é—á–∏ Google –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã!")
       return []

   
   try:
       logging.info(f"üîç google_search: –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ API")
       response = requests.get(url, params=params, timeout=15)
       status_code = response.status_code
       logging.info(f"üîç google_search: –°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞: {status_code}")
       
       response.raise_for_status()
       data = response.json()
       
       # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
       if "items" not in data:
           logging.warning(f"‚ö†Ô∏è google_search: API –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
           logs.append(f"‚ö†Ô∏è Google –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{clean_query}'")
           return []
       
       items = data.get("items", [])
       logging.info(f"üîç google_search: –ù–∞–π–¥–µ–Ω–æ {len(items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
       links = [item.get("link") for item in items if item.get("link")]
       
       # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ URL
       valid_links = [link for link in links if is_valid_url(link)]
       
       elapsed_time = time.time() - start_time
       logging.info(f"‚úÖ google_search: –ù–∞–π–¥–µ–Ω–æ {len(valid_links)} –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
       logs.append(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(valid_links)} —Å—Å—ã–ª–æ–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{clean_query}' –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")
       
       # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
       for i, link in enumerate(valid_links[:3]):
           logging.info(f"URL {i+1}: {link}")
           logs.append(f"URL {i+1}: {link}")
       
       return valid_links
       
   except requests.exceptions.RequestException as e:
       logs.append(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ Google: {str(e)}")
       logger.error(f"–û—à–∏–±–∫–∞ Google API: {str(e)}")
       return []

def is_valid_url(url: str) -> bool:
   """
   –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –¥–æ–ø—É—Å—Ç–∏–º—ã–º.
   
   Args:
       url (str): URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
       
   Returns:
       bool: True, –µ—Å–ª–∏ URL –¥–æ–ø—É—Å—Ç–∏–º, –∏–Ω–∞—á–µ False
   """
   try:
       result = urlparse(url)
       return all([result.scheme, result.netloc])
   except:
       return False

def prioritize_links(links: List[str], query: str) -> List[str]:
   """
   –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ—Ç —Å—Å—ã–ª–∫–∏ –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å—É.
   
   Args:
       links: –°–ø–∏—Å–æ–∫ URL
       query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
       
   Returns:
       –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ URL —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
   """
   # –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã –¥–ª—è –ø—Ä–∞–≤–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
   high_quality_domains = [
       "consultant.ru", "garant.ru", "sudact.ru", "pravo.gov.ru", 
       "zakon.ru", "ksrf.ru", "vsrf.ru", "arbitr.ru"
   ]
   
   # –û—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥–æ–π —Å—Å—ã–ª–∫–∏
   scored_links = []
   
   for url in links:
       score = 0
       parsed_url = urlparse(url)
       domain = parsed_url.netloc.lower()
       
       # –ü–æ–≤—ã—à–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
       if any(good_domain in domain for good_domain in high_quality_domains):
           score += 10
       
       # –ü–æ–≤—ã—à–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –ø—Ä–∞–≤–æ–≤—ã—Ö –¥–æ–º–µ–Ω–æ–≤
       if any(term in domain for term in ["garant", "consultant", "pravorub", "pravo", "sudact", "zakon"]):
           score += 5
       
       # –û—Ü–µ–Ω–∏–≤–∞–µ–º –ø–æ –Ω–∞–ª–∏—á–∏—é –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –≤ URL
       query_terms = query.lower().split()
       for term in query_terms:
           if len(term) > 3 and term in url.lower():
               score += 2
       
       # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã PDF, DOCX –∏ —Ç.–¥. –¥–ª—è –ø—Ä–∞–≤–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
       if url.lower().endswith(('.pdf', '.doc', '.docx')):
           score += 3
       
       # –†—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ —Å–∞–π—Ç—ã –ø–æ–ª—É—á–∞—é—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
       if ".ru" in domain or "—Ä—Ñ" in domain:
           score += 5
       
       scored_links.append((url, score))
   
   # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ –ø–æ —É–±—ã–≤–∞–Ω–∏—é –æ—Ü–µ–Ω–∫–∏
   scored_links.sort(key=lambda x: x[1], reverse=True)
   
   # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ URL, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
   return [url for url, _ in scored_links]


async def search_and_scrape(query: str, logs: list, max_results: int = 3, force_refresh: bool = False) -> list:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ Google, –ø–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–µ–±-—Å—Å—ã–ª–æ–∫ –∏ –ø–µ—Ä–µ–¥–∞—ë—Ç –∏—Ö –≤ –º–æ–¥—É–ª—å —Å–∫—Ä–µ–π–ø–µ—Ä–∞ 
    –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü. –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    
    Args:
        query (str): –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
        logs (list): —Å–ø–∏—Å–æ–∫ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è.
        max_results (int): –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3).
        force_refresh (bool): —Ñ–ª–∞–≥ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫—ç—à–∞.
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ ScrapedContent, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.
    """
    start_time = time.time()
    logs.append(f"üîç –ù–∞—á–∞–ª–æ –ø–æ–∏—Å–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –∏ –ø–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ (–Ω–µ –±–æ–ª–µ–µ —á–µ–º –Ω–∞–º –Ω—É–∂–Ω–æ)
    links = google_search(query, logs, max_results=max_results) 
    
    if not links:
        logs.append("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞")
        return []
    
    try:
        # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏
        prioritized_links = prioritize_links(links, query)
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫
        links_to_scrape = prioritized_links[:max_results]
        logs.append(f"üì• –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∏–∑ {len(links_to_scrape)} —Å—Å—ã–ª–æ–∫...")

        if not links:
            logging.warning(f"‚ö†Ô∏è search_and_scrape: –°—Å—ã–ª–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è '{query}'")
            return []
        
        successful_results = []
        for url in links_to_scrape:
            try:
                # Check if the URL points to a binary file
                is_binary = url.lower().endswith(('.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx'))
                
                if is_binary:
                    # Handle binary files differently
                    logs.append(f"üìÑ Detected binary file: {url}")
                    
                    # For PDFs, create a placeholder ScrapedContent with metadata
                    if url.lower().endswith('.pdf'):
                        # Create placeholder with file info but don't try to decode content
                        result = ScrapedContent(
                            url=url,
                            title=f"PDF Document: {url.split('/')[-1]}",
                            text=f"Binary PDF file available at: {url}",
                            html="",
                            metadata={"binary_type": "pdf", "scraped": False},
                            content_type="application/pdf"
                        )
                        successful_results.append(result)
                        logs.append(f"‚úÖ Created placeholder for binary PDF: {url}")
                    else:
                        # For other binary types, create appropriate placeholders
                        file_type = url.split('.')[-1].upper()
                        result = ScrapedContent(
                            url=url,
                            title=f"{file_type} Document: {url.split('/')[-1]}",
                            text=f"Binary {file_type} file available at: {url}",
                            html="",
                            metadata={"binary_type": file_type.lower(), "scraped": False},
                            content_type=f"application/{file_type.lower()}"
                        )
                        successful_results.append(result)
                        logs.append(f"‚úÖ Created placeholder for binary {file_type}: {url}")
                else:
                    # Use existing scraper for HTML content
                    scraper = get_scraper()
                    result = await scraper.scrape_url(url, dynamic=False)
                    
                    if result.is_successful():
                        successful_results.append(result)
                        logs.append(f"‚úÖ Successfully scraped: {url}")
                    else:
                        logs.append(f"‚ùå Failed to scrape: {url} - {result.error}")
            except Exception as e:
                logs.append(f"‚ùå Error processing URL {url}: {str(e)}")
                logger.error(f"Error processing URL {url}: {str(e)}")
        
        return successful_results
        
    except Exception as e:
        logs.append(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–µ: {str(e)}")
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–µ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}': {str(e)}")
        return []


async def run_multiple_searches(query: str, logs: list, force_refresh: bool = False) -> Dict[str, List]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø–æ–∏—Å–∫ –≤–º–µ—Å—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–æ 3.
    """
    logs.append(f"üîÑ –ó–∞–ø—É—Å–∫ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø–æ–∏—Å–∫
    try:
        general_results = await search_and_scrape(query, logs, max_results=3, force_refresh=force_refresh)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å–ª–æ–≤–∞—Ä—å 
        results = {
            "general": general_results,
            # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏
            "legal": [],
            "recent": []
        }
        
        total_results = len(general_results)
        logs.append(f"üìä –ü–æ–ª—É—á–µ–Ω–æ –≤—Å–µ–≥–æ {total_results} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ –ø–æ–∏—Å–∫–∞")
        
        return results
    
    except Exception as e:
        logs.append(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø–æ–∏—Å–∫–∞: {str(e)}")
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø–æ–∏—Å–∫–∞: {str(e)}")
        return {"legal": [], "recent": [], "general": []}