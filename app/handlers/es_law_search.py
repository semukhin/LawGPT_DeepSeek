# app/handlers/es_law_search.py

import logging
from typing import List, Dict, Any, Optional
from elasticsearch import Elasticsearch
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Elasticsearch
ES_HOST = "http://localhost:9200"
ES_USER = "elastic"
ES_PASS = "GIkb8BKzkXK7i2blnG2O"

# –ò–Ω–¥–µ–∫—Å—ã –≤ Elasticsearch
ES_INDICES = {
    "court_decisions": "court_decisions_index",
    "court_reviews": "court_reviews_index",
    "legal_articles": "legal_articles_index",
    "ruslawod_chunks": "ruslawod_chunks_index"
}

def get_es_client():
    """
    –°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç Elasticsearch.
    
    Returns:
        Elasticsearch: –ö–ª–∏–µ–Ω—Ç Elasticsearch
    """
    try:
        es = Elasticsearch(
            [ES_HOST],
            basic_auth=(ES_USER, ES_PASS),
            retry_on_timeout=True,
            max_retries=3,
            request_timeout=30
        )
        logger.info("‚úÖ –£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Elasticsearch")
        return es
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Elasticsearch: {str(e)}")
        raise

def search_law_chunks(query: str, top_n: int = 7) -> List[str]:
    """
    –ü–æ–∏—Å–∫ –≤ Elasticsearch –ø–æ –≤—Å–µ–º –∏–Ω–¥–µ–∫—Å–∞–º.
    
    Args:
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        top_n: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞
    """
    try:
        logger.info(f"üîç [ES] –ù–∞—á–∞–ª–æ –ø–æ–∏—Å–∫–∞ –≤ Elasticsearch –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
        es = get_es_client()
        
        results = []
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏
        court_decisions_limit = max(2, top_n // 4)
        court_reviews_limit = max(1, top_n // 4)
        legal_articles_limit = max(1, top_n // 4)
        ruslawod_chunks_limit = top_n - court_decisions_limit - court_reviews_limit - legal_articles_limit
        
        # 1. –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ court_decisions (—Å—É–¥–µ–±–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è)
        court_decision_results = search_court_decisions(es, query, court_decisions_limit)
        results.extend(court_decision_results)
        
        # 2. –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ ruslawod_chunks (—á–∞–Ω–∫–∏ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞)
        ruslawod_chunks_results = search_ruslawod_chunks(es, query, ruslawod_chunks_limit)
        results.extend(ruslawod_chunks_results)
        
        # 3. –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ court_reviews (–æ–±–∑–æ—Ä—ã —Å—É–¥–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π)
        court_reviews_results = search_court_reviews(es, query, court_reviews_limit)
        results.extend(court_reviews_results)
        
        # 4. –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ legal_articles (–ø—Ä–∞–≤–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏)
        legal_articles_results = search_legal_articles(es, query, legal_articles_limit)
        results.extend(legal_articles_results)
        
        logger.info(f"üîç [ES] –ù–∞–π–¥–µ–Ω–æ –≤—Å–µ–≥–æ {len(results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.")
        return results
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ Elasticsearch: {str(e)}")
        return []

def search_court_decisions(es, query: str, limit: int) -> List[str]:
    """
    –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ court_decisions (—Å—É–¥–µ–±–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è).
    
    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        List[str]: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        index_name = ES_INDICES.get("court_decisions", "court_decisions_index")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if not es.indices.exists(index=index_name):
            logger.warning(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
            return []
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        body = {
            "size": limit,
            "query": {
                "bool": {
                    "should": [
                        {
                            "match": {
                                "case_number": query
                            }
                        },
                        {
                            "multi_match": {
                                "query": query,
                                "fields": [
                                    "case_number^5",
                                    "judges^3",
                                    "claimant^3",
                                    "defendant^3",
                                    "subject^2",
                                    "arguments",
                                    "conclusion",
                                    "full_text"
                                ],
                                "type": "best_fields",
                                "fuzziness": "AUTO"
                            }
                        }
                    ],
                    "minimum_should_match": 1
                }
            },
            "highlight": {
                "fields": {
                    "full_text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]}
                },
                "fragment_size": 300,
                "number_of_fragments": 3
            }
        }
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        response = es.search(index=index_name, body=body)
        hits = response["hits"]["hits"]
        
        results = []
        for hit in hits:
            source = hit["_source"]
            case_number = source.get("case_number", "")
            subject = source.get("subject", "")
            judges = source.get("judges", "")
            claimant = source.get("claimant", "")
            defendant = source.get("defendant", "")
            full_text = source.get("full_text", "")
            
            highlights = hit.get("highlight", {}).get("full_text", [])
            highlight_text = "...\n".join(highlights)
            
            result = f"–°—É–¥–µ–±–Ω–æ–µ –¥–µ–ª–æ ‚Ññ {case_number}\n"
            result += f"–°—É–¥—å–∏: {judges}\n–ò—Å—Ç–µ—Ü: {claimant}\n–û—Ç–≤–µ—Ç—á–∏–∫: {defendant}\n–ü—Ä–µ–¥–º–µ—Ç: {subject}\n"
            
            if highlights:
                result += f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n{highlight_text}\n\n"
            
            result += f"–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç:\n{full_text[:2000]}..."
            
            results.append(result)
        
        logger.info(f"üîç [ES] –ù–∞–π–¥–µ–Ω–æ {len(results)} —Å—É–¥–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π")
        return results
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ court_decisions: {str(e)}")
        return []

def search_ruslawod_chunks(es, query: str, limit: int) -> List[str]:
    """
    –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ ruslawod_chunks (—á–∞–Ω–∫–∏ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞).
    
    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        List[str]: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        index_name = ES_INDICES.get("ruslawod_chunks", "ruslawod_chunks_index")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if not es.indices.exists(index=index_name):
            logger.warning(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
            return []
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        body = {
            "size": limit,
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": [
                        "text^3",
                        "content^3",
                        "full_text^2",
                        "law_name",
                        "section_name",
                        "article_name"
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            },
            "highlight": {
                "fields": {
                    "text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "content": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "full_text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]}
                },
                "fragment_size": 300,
                "number_of_fragments": 3
            }
        }
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        response = es.search(index=index_name, body=body)
        hits = response["hits"]["hits"]
        
        results = []
        for hit in hits:
            source = hit["_source"]
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
            law_name = source.get("law_name", "")
            section = source.get("section_name", source.get("section", ""))
            article = source.get("article_name", source.get("article", ""))
            content = source.get("content", source.get("text", source.get("full_text", "")))
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
            document_id = source.get("document_id", "")
            chunk_id = source.get("chunk_id", "")
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥—Å–≤–µ—á–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            highlights = []
            for field in ["text", "content", "full_text"]:
                if hit.get("highlight", {}).get(field):
                    highlights.extend(hit["highlight"][field])
            
            highlight_text = "...\n".join(highlights) if highlights else ""
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = f"–ó–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ: {law_name}\n"
            
            if section:
                result += f"–†–∞–∑–¥–µ–ª: {section}\n"
            
            if article:
                result += f"–°—Ç–∞—Ç—å—è: {article}\n"
            
            if document_id and chunk_id:
                result += f"ID –¥–æ–∫—É–º–µ–Ω—Ç–∞: {document_id}, ID —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞: {chunk_id}\n"
            
            if highlight_text:
                result += f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n{highlight_text}\n\n"
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            if content:
                content_preview = content[:2000] + "..." if len(content) > 2000 else content
                result += f"–¢–µ–∫—Å—Ç:\n{content_preview}"
            
            results.append(result)
        
        logger.info(f"üîç [ES] –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞")
        return results
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ ruslawod_chunks: {str(e)}")
        return []

def search_court_reviews(es, query: str, limit: int) -> List[str]:
    """
    –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ court_reviews (–æ–±–∑–æ—Ä—ã —Å—É–¥–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π).
    
    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        List[str]: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        index_name = ES_INDICES.get("court_reviews", "court_reviews_index")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if not es.indices.exists(index=index_name):
            logger.warning(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
            return []
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        body = {
            "size": limit,
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": [
                        "title^4",
                        "content^3",
                        "description^2",
                        "text",
                        "full_text"
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            },
            "highlight": {
                "fields": {
                    "title": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "content": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "full_text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]}
                },
                "fragment_size": 300,
                "number_of_fragments": 3
            }
        }
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        response = es.search(index=index_name, body=body)
        hits = response["hits"]["hits"]
        
        results = []
        for hit in hits:
            source = hit["_source"]
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
            title = source.get("title", "–û–±–∑–æ—Ä —Å—É–¥–µ–±–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏")
            content = source.get("content", source.get("text", source.get("full_text", "")))
            author = source.get("author", "")
            publication_date = source.get("publication_date", source.get("date", ""))
            source_name = source.get("source", "")
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥—Å–≤–µ—á–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            highlights = []
            for field in ["title", "content", "text", "full_text"]:
                if hit.get("highlight", {}).get(field):
                    highlights.extend(hit["highlight"][field])
            
            highlight_text = "...\n".join(highlights) if highlights else ""
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = f"–û–±–∑–æ—Ä —Å—É–¥–µ–±–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏: {title}\n"
            
            if author:
                result += f"–ê–≤—Ç–æ—Ä: {author}\n"
            
            if publication_date:
                result += f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {publication_date}\n"
            
            if source_name:
                result += f"–ò—Å—Ç–æ—á–Ω–∏–∫: {source_name}\n"
            
            if highlight_text:
                result += f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n{highlight_text}\n\n"
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            if content:
                content_preview = content[:2000] + "..." if len(content) > 2000 else content
                result += f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n{content_preview}"
            
            results.append(result)
        
        logger.info(f"üîç [ES] –ù–∞–π–¥–µ–Ω–æ {len(results)} –æ–±–∑–æ—Ä–æ–≤ —Å—É–¥–µ–±–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏")
        return results
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ court_reviews: {str(e)}")
        return []

def search_legal_articles(es, query: str, limit: int) -> List[str]:
    """
    –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ legal_articles (–ø—Ä–∞–≤–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏).
    
    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        List[str]: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        index_name = ES_INDICES.get("legal_articles", "legal_articles_index")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if not es.indices.exists(index=index_name):
            logger.warning(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
            return []
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        body = {
            "size": limit,
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": [
                        "title^4",
                        "content^3",
                        "description^2",
                        "body",
                        "text"
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            },
            "highlight": {
                "fields": {
                    "title": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "content": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "body": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]}
                },
                "fragment_size": 300,
                "number_of_fragments": 3
            }
        }
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        response = es.search(index=index_name, body=body)
        hits = response["hits"]["hits"]
        
        results = []
        for hit in hits:
            source = hit["_source"]
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
            title = source.get("title", "–ü—Ä–∞–≤–æ–≤–∞—è —Å—Ç–∞—Ç—å—è")
            content = source.get("content", source.get("body", source.get("text", "")))
            author = source.get("author", "")
            publication_date = source.get("publication_date", source.get("date", ""))
            source_name = source.get("source", "")
            tags = source.get("tags", source.get("keywords", ""))
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥—Å–≤–µ—á–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            highlights = []
            for field in ["title", "content", "body", "text"]:
                if hit.get("highlight", {}).get(field):
                    highlights.extend(hit["highlight"][field])
            
            highlight_text = "...\n".join(highlights) if highlights else ""
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = f"–ü—Ä–∞–≤–æ–≤–∞—è —Å—Ç–∞—Ç—å—è: {title}\n"
            
            if author:
                result += f"–ê–≤—Ç–æ—Ä: {author}\n"
            
            if publication_date:
                result += f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {publication_date}\n"
            
            if source_name:
                result += f"–ò—Å—Ç–æ—á–Ω–∏–∫: {source_name}\n"
            
            if tags:
                tags_str = tags if isinstance(tags, str) else ", ".join(tags)
                result += f"–¢–µ–≥–∏: {tags_str}\n"
            
            if highlight_text:
                result += f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n{highlight_text}\n\n"
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            if content:
                content_preview = content[:2000] + "..." if len(content) > 2000 else content
                result += f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n{content_preview}"
            
            results.append(result)
        
        logger.info(f"üîç [ES] –ù–∞–π–¥–µ–Ω–æ {len(results)} –ø—Ä–∞–≤–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π")
        return results
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ legal_articles: {str(e)}")
        return []

def create_court_decisions_index(es):
    """
    –°–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å court_decisions –≤ Elasticsearch, –µ—Å–ª–∏ –æ–Ω –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
    
    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
    """
    index_name = ES_INDICES.get("court_decisions", "court_decisions_index")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
    if not es.indices.exists(index=index_name):
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞
        mappings = {
            "properties": {
                "case_number": {"type": "text", "analyzer": "russian"},
                "judges": {"type": "text", "analyzer": "russian"},
                "claimant": {"type": "text", "analyzer": "russian"},
                "defendant": {"type": "text", "analyzer": "russian"},
                "subject": {"type": "text", "analyzer": "russian"},
                "arguments": {"type": "text", "analyzer": "russian"},
                "conclusion": {"type": "text", "analyzer": "russian"},
                "full_text": {"type": "text", "analyzer": "russian"}
            }
        }
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
        es.indices.create(
            index=index_name,
            body={
                "settings": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "analysis": {
                        "analyzer": {
                            "russian": {
                                "tokenizer": "standard",
                                "filter": ["lowercase", "russian_morphology", "russian_stop"]
                            }
                        }
                    }
                },
                "mappings": mappings
            }
        )
        
        logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å {index_name} —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω.")
    else:
        logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å {index_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")

def create_ruslawod_chunks_index(es):
    """
    –°–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å ruslawod_chunks –≤ Elasticsearch, –µ—Å–ª–∏ –æ–Ω –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
    
    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
    """
    index_name = ES_INDICES.get("ruslawod_chunks", "ruslawod_chunks_index")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
    if not es.indices.exists(index=index_name):
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞
        mappings = {
            "properties": {
                "law_name": {"type": "text", "analyzer": "russian"},
                "section_name": {"type": "text", "analyzer": "russian"},
                "article_name": {"type": "text", "analyzer": "russian"},
                "document_id": {"type": "keyword"},
                "chunk_id": {"type": "keyword"},
                "content": {"type": "text", "analyzer": "russian"},
                "text": {"type": "text", "analyzer": "russian"},
                "full_text": {"type": "text", "analyzer": "russian"}
            }
        }
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
        es.indices.create(
            index=index_name,
            body={
                "settings": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "analysis": {
                        "analyzer": {
                            "russian": {
                                "tokenizer": "standard",
                                "filter": ["lowercase", "russian_morphology", "russian_stop"]
                            }
                        }
                    }
                },
                "mappings": mappings
            }
        )
        
        logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å {index_name} —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω.")
    else:
        logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å {index_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")

def create_indices():
    """
    –°–æ–∑–¥–∞–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–Ω–¥–µ–∫—Å—ã –≤ Elasticsearch.
    """
    try:
        es = get_es_client()
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
        create_court_decisions_index(es)
        create_ruslawod_chunks_index(es)
        
        # –¢–∞–∫–∂–µ –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è court_reviews –∏ legal_articles,
        # –Ω–æ –ø–æ–∫–∞ –æ—Å—Ç–∞–≤–∏–º –∏—Ö –±–µ–∑ —è–≤–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è
        
        logger.info("‚úÖ –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–Ω–¥–µ–∫—Å—ã –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –∏–ª–∏ —Å–æ–∑–¥–∞–Ω—ã.")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–æ–≤: {str(e)}")

def index_court_decisions():
    """
    –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å—É–¥–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –≤ Elasticsearch.
    –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π.
    """
    create_indices()

if __name__ == "__main__":
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –º–æ–¥—É–ª—è –Ω–∞–ø—Ä—è–º—É—é
    create_indices()
    
    # –ü—Ä–∏–º–µ—Ä –ø–æ–∏—Å–∫–∞
    test_query = "–ê65-28469/2012"
    results = search_law_chunks(test_query)
    print(f"–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{test_query}': –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    if results:
        print("\n–ü–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(results[0][:500] + "...")