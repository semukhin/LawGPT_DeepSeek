from typing import List, Dict, Any, Optional
from elasticsearch import Elasticsearch
import re
import os
import json
from app.config import ELASTICSEARCH_URL, ES_INDICES as CONFIG_ES_INDICES
from app.utils.logger import get_logger, LogLevel
from app.services.embedding_service import EmbeddingService

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–≥–µ—Ä
logger = get_logger()

ES_HOST = os.getenv("ES_HOST", "http://localhost:9200")
ES_USER = os.getenv("ES_USER", None)
ES_PASS = os.getenv("ES_PASS", None)


# –ò–Ω–¥–µ–∫—Å—ã –≤ Elasticsearch —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
DEFAULT_ES_INDICES = {
    "court_decisions": "court_decisions_index",
    "court_reviews": "court_reviews_index",
    "legal_articles": "legal_articles_index",
    "ruslawod_chunks": "ruslawod_chunks_index",
    "procedural_forms": "procedural_forms_index"
}

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
ES_INDICES = CONFIG_ES_INDICES or DEFAULT_ES_INDICES

def get_es_client():
    """
    –°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç Elasticsearch.

    Returns:
        Elasticsearch: –ö–ª–∏–µ–Ω—Ç Elasticsearch
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–∞ –ª–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        if ES_USER and ES_PASS and ES_USER.lower() != 'none' and ES_PASS.lower() != 'none':
            # –° –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π
            logger.log("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Elasticsearch —Å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π", LogLevel.INFO)
            es = Elasticsearch(
                ELASTICSEARCH_URL,
                basic_auth=(ES_USER, ES_PASS),
                retry_on_timeout=True,
                max_retries=3
            )
        else:
            # –ë–µ–∑ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
            logger.log("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Elasticsearch –±–µ–∑ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏", LogLevel.INFO)
            es = Elasticsearch(
                ELASTICSEARCH_URL,
                retry_on_timeout=True,
                max_retries=3
            )
        return es
    except Exception as e:
        logger.log(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Elasticsearch: {e}", LogLevel.ERROR)
        raise


# –£–º–Ω—ã–π –ø–æ–∏—Å–∫ - –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
class SmartSearchService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ Elasticsearch"""

    def __init__(self, es_client):
        self.es = es_client
        self.court_decisions_index = ES_INDICES.get("court_decisions", "court_decisions_index")
        self.court_reviews_index = ES_INDICES.get("court_reviews", "court_reviews_index")
        self.legal_articles_index = ES_INDICES.get("legal_articles", "legal_articles_index")
        self.ruslawod_chunks_index = ES_INDICES.get("ruslawod_chunks", "ruslawod_chunks_index")
        self.procedural_forms_index = ES_INDICES.get("procedural_forms", "procedural_forms_index")


    def extract_case_number(self, query: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä –¥–µ–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–º–µ—Ä–æ–≤ –¥–µ–ª —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
        pattern = r'[–êA]\d{1,2}-\d+/\d{2,4}(?:-[–ê-–Ø–∞-—èA-Za-z0-9]+)*'

        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–µ
        if isinstance(query, bytes):
            query = query.decode('utf-8')

        match = re.search(pattern, query)
        if match:
            case_number = match.group(0)
            logger.log(f"SmartSearchService: –ò–∑–≤–ª–µ—á–µ–Ω –Ω–æ–º–µ—Ä –¥–µ–ª–∞: {case_number}", LogLevel.INFO)

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≥–æ–¥, –µ—Å–ª–∏ –æ–Ω –≤ –∫–æ—Ä–æ—Ç–∫–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 05 -> 2005)
            parts = case_number.split('/')
            if len(parts) > 1:
                year_part = parts[1].split('-')[0]  # –±–µ—Ä–µ–º –≥–æ–¥ –¥–æ –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ —Å—É—Ñ—Ñ–∏–∫—Å–∞
                if len(year_part) == 2:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º 2-–∑–Ω–∞—á–Ω—ã–π –≥–æ–¥ –≤ 4-–∑–Ω–∞—á–Ω—ã–π (05 -> 2005)
                    full_year = f"20{year_part}" if int(year_part) < 50 else f"19{year_part}"
                    case_number = case_number.replace(f"/{year_part}", f"/{full_year}", 1)
                    logger.log(f"SmartSearchService: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω –≥–æ–¥ –≤ –Ω–æ–º–µ—Ä–µ –¥–µ–ª–∞: {case_number}", LogLevel.INFO)

            return case_number
        logger.log(f"SmartSearchService: –ù–æ–º–µ—Ä –¥–µ–ª–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∑–∞–ø—Ä–æ—Å–µ: '{query}'", LogLevel.INFO)
        return None


    def extract_company_name(self, query: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        # –ü–æ–∏—Å–∫ –ø–æ —à–∞–±–ª–æ–Ω–∞–º "–û–û–û", "–ó–ê–û", "–û–ê–û", "–ü–ê–û" –∏ —Ç.–¥.
        patterns = [
            r'–û–û–û\s+[¬´"]?([^¬ª"]+)[¬ª"]?',
            r'–ó–ê–û\s+[¬´"]?([^¬ª"]+)[¬ª"]?',
            r'–û–ê–û\s+[¬´"]?([^¬ª"]+)[¬ª"]?',
            r'–ü–ê–û\s+[¬´"]?([^¬ª"]+)[¬ª"]?',
            r'–ò–ü\s+([–ê-–Ø–∞-—è\s]+)',
            r'–û–ì–†–ù\s+(\d{13}|\d{15})',
            r'–ò–ù–ù\s+(\d{10}|\d{12})'
        ]

        for pattern in patterns:
            match = re.search(pattern, query)
            if match:
                return match.group(0)
        return None

    def extract_document_type(self, query: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        logger.log(f"üîé –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'", LogLevel.INFO)
        doc_types = [
                "–∏—Å–∫–æ–≤–æ–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ", "–∏—Å–∫", "–ø—Ä–µ—Ç–µ–Ω–∑–∏—è", "–æ—Ç–∑—ã–≤", "–æ—Ç–∑—ã–≤ –Ω–∞ –∏—Å–∫–æ–≤–æ–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ", 
                "—Ö–æ–¥–∞—Ç–∞–π—Å—Ç–≤–æ", "–∞–ø–µ–ª–ª—è—Ü–∏–æ–Ω–Ω–∞—è –∂–∞–ª–æ–±–∞", "–∫–∞—Å—Å–∞—Ü–∏–æ–Ω–Ω–∞—è –∂–∞–ª–æ–±–∞", 
                "–∑–∞—è–≤–ª–µ–Ω–∏–µ", "–≤–æ–∑—Ä–∞–∂–µ–Ω–∏–µ", "–¥–æ–≥–æ–≤–æ—Ä", "—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ", "–∂–∞–ª–æ–±–∞", "–º–∏—Ä–æ–≤–æ–µ —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ",
                "—Å–æ–≥–ª–∞—Å–∏–µ", "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–æ–µ –∏—Å–∫–æ–≤–æ–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ", "–∑–∞–º–µ—á–∞–µ–Ω–∏–µ", "–æ—Ç–≤–µ—Ç", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∫ –∏—Å–∫–æ–≤–æ–º—É –∑–∞—è–≤–ª–µ–Ω–∏—é",
                "—Ä–∞—Å–ø–∏—Å–∫–∞", "—Ä–∞—Å—á–µ—Ç", "–∫–æ–Ω—Ç—Ä—Ä–∞—Å—á–µ—Ç", "–æ—Ç–≤–µ—Ç –Ω–∞ –ø—Ä–µ—Ç–µ–Ω–∑–∏—é", "–∑–∞–º–µ—á–∞–Ω–∏—è –Ω–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª", "–Ω–µ–∑–∞–≤–∏—Å–∏–º–∞—è –≥–∞—Ä–∞–Ω—Ç–∏—è",
                "–æ—Ç–≤–µ—Ç –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É–¥–∞", "—Ä–∞—Å—á–µ—Ç –∏—Å–∫–æ–≤—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π", "—Ä–∞—Å—á–µ—Ç —É–±—ã—Ç–∫–æ–≤"
            ]

        for doc_type in doc_types:
            if doc_type.lower() in query.lower():
                logger.log(f"üîé –ù–∞–π–¥–µ–Ω —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: '{doc_type}'", LogLevel.INFO)
                return doc_type
        logger.log(f"üîé –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω", LogLevel.INFO)
        return None

    def search_by_case_number(self, case_number: str, limit: int = 10) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—É–¥–µ–±–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –ø–æ –Ω–æ–º–µ—Ä—É –¥–µ–ª–∞"""
        logger.log(f"üîç –ü–æ–∏—Å–∫ –ø–æ –Ω–æ–º–µ—Ä—É –¥–µ–ª–∞: {case_number}", LogLevel.INFO)

        # –†–∞–∑–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä –¥–µ–ª–∞ –Ω–∞ —Å–æ—Å—Ç–∞–≤–Ω—ã–µ —á–∞—Å—Ç–∏
        full_case_number = case_number

        # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é —á–∞—Å—Ç—å –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞ (–±–µ–∑ —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤ –ø–æ—Å–ª–µ –≥–æ–¥–∞)
        base_parts = full_case_number.split('-')
        court_code = base_parts[0]  # A40 –∏–ª–∏ –ê40

        # –°–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –∞–ª—Ñ–∞–≤–∏—Ç–∞–º–∏
        variants = []

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π –Ω–æ–º–µ—Ä —Å –æ–±–æ–∏–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –±—É–∫–≤—ã –ê/A
        variants.append(full_case_number)
        if '–ê' in full_case_number:
            variants.append(full_case_number.replace('–ê', 'A'))
        else:
            variants.append(full_case_number.replace('A', '–ê'))

        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—É—Ñ—Ñ–∏–∫—Å—ã –ø–æ—Å–ª–µ –≥–æ–¥–∞, –¥–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –±–µ–∑ –Ω–∏—Ö
        if len(base_parts) > 2:
            year_parts = base_parts[1].split('/')
            if len(year_parts) > 1:
                base_case_number = f"{court_code}-{base_parts[1]}"
                variants.append(base_case_number)
                if '–ê' in base_case_number:
                    variants.append(base_case_number.replace('–ê', 'A'))
                else:
                    variants.append(base_case_number.replace('A', '–ê'))

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        variants = list(set(variants))
        logger.log(f"üîç –í–∞—Ä–∏–∞–Ω—Ç—ã –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞: {variants}", LogLevel.INFO)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
        should_clauses = []
        for variant in variants:
            should_clauses.append({"term": {"case_number": variant}})
            should_clauses.append({"match_phrase": {"full_text": variant}})

        body = {
            "query": {
                "bool": {
                    "should": should_clauses,
                    "minimum_should_match": 1
                }
            },
            "size": limit,
            "sort": [
                {"doc_id": {"order": "asc"}},
                {"chunk_id": {"order": "asc"}}
            ]
        }

        try:
            # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            logger.log(f"üîç –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å: {json.dumps(body, ensure_ascii=False)}", LogLevel.INFO)

            response = self.es.search(index=self.court_decisions_index, body=body)
            hits = response["hits"]["hits"]

            if hits:
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                results = []
                doc_id_set = set()

                for hit in hits:
                    source = hit["_source"]
                    doc_id = source.get("doc_id", "")
                    doc_id_set.add(doc_id)
                    results.append(source)

                logger.log(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –¥–µ–ª–∞ {case_number} (–¥–æ–∫—É–º–µ–Ω—Ç—ã: {len(doc_id_set)})", LogLevel.INFO)
                return results
            else:
                logger.log(f"üîç –î–µ–ª–æ {case_number} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ", LogLevel.WARNING)
                return []

        except Exception as e:
            logger.log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–µ–ª–∞ {case_number}: {str(e)}", LogLevel.ERROR)
            logger.exception("Stacktrace:")
            return []

    def search_by_company(self, company: str, limit: int = 10) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –¥–µ–ª —Å —É—á–∞—Å—Ç–∏–µ–º —É–∫–∞–∑–∞–Ω–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏"""
        logger.log(f"üîç –ü–æ–∏—Å–∫ –¥–µ–ª —Å —É—á–∞—Å—Ç–∏–µ–º –∫–æ–º–ø–∞–Ω–∏–∏: {company}", LogLevel.INFO)

        body = {
            "size": limit,
            "query": {
                "bool": {
                    "should": [
                        {"match": {"claimant": company}},
                        {"match": {"defendant": company}},
                        {"match": {"full_text": company}}
                    ],
                    "minimum_should_match": 1
                }
            },
            "collapse": {
                "field": "doc_id"  # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
            },
            "_source": ["doc_id", "case_number", "court_name", "date", "subject", "claimant", "defendant"]
        }

        try:
            response = self.es.search(index=self.court_decisions_index, body=body)
            hits = response["hits"]["hits"]

            if hits:
                results = [hit["_source"] for hit in hits]
                logger.log(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} –¥–µ–ª —Å —É—á–∞—Å—Ç–∏–µ–º {company}", LogLevel.INFO)
                return results
            else:
                logger.log(f"üîç –î–µ–ª–∞ —Å —É—á–∞—Å—Ç–∏–µ–º {company} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã", LogLevel.WARNING)
                return []

        except Exception as e:
            logger.log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∫–æ–º–ø–∞–Ω–∏–∏ {company}: {str(e)}", LogLevel.ERROR)
            return []

    def search_by_text_fragment(self, text: str, limit: int = 10) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Ç–µ–∫—Å—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (–ø—Ä–µ–¥—ã–¥—É—â–∏–π –∏ —Å–ª–µ–¥—É—é—â–∏–π —á–∞–Ω–∫–∏)"""
        logger.log(f"üîç –ü–æ–∏—Å–∫ –ø–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—É —Ç–µ–∫—Å—Ç–∞: {text[:100]}...", LogLevel.INFO)

        body = {
            "size": limit,
            "query": {
                "match_phrase": {
                    "full_text": text
                }
            },
            "highlight": {
                "fields": {
                    "full_text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]}
                }
            }
        }

        try:
            response = self.es.search(index=self.court_decisions_index, body=body)
            hits = response["hits"]["hits"]

            if not hits:
                logger.log(f"üîç –§—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω", LogLevel.WARNING)
                return []

            results = []

            for hit in hits:
                source = hit["_source"]
                doc_id = source.get("doc_id", "")
                chunk_id = source.get("chunk_id", 0)

                # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –∏ —Å–ª–µ–¥—É—é—â–∏–π —á–∞–Ω–∫–∏
                prev_chunk = self._get_adjacent_chunk(doc_id, chunk_id - 1)
                next_chunk = self._get_adjacent_chunk(doc_id, chunk_id + 1)

                # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                result = {
                    "current": source,
                    "highlight": hit.get("highlight", {}).get("full_text", []),
                    "prev": prev_chunk,
                    "next": next_chunk
                }

                results.append(result)

            logger.log(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", LogLevel.INFO)
            return results

        except Exception as e:
            logger.log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Ç–µ–∫—Å—Ç–∞: {str(e)}", LogLevel.ERROR)
            return []

    def _get_adjacent_chunk(self, doc_id: str, chunk_id: int) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–º–µ–∂–Ω—ã–π —á–∞–Ω–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        if chunk_id < 1:
            return None

        body = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"doc_id": doc_id}},
                        {"term": {"chunk_id": chunk_id}}
                    ]
                }
            }
        }

        try:
            response = self.es.search(index=self.court_decisions_index, body=body)
            hits = response["hits"]["hits"]

            if hits:
                return hits[0]["_source"]
            return None

        except Exception:
            return None

    def smart_search(self, query: str, limit: int = 10) -> Dict[str, Any]:
        """–£–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –∏ –≤—ã–±–æ—Ä –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –º–µ—Ç–æ–¥–∞ –ø–æ–∏—Å–∫–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞ –≤ –∑–∞–ø—Ä–æ—Å–µ
        case_number = self.extract_case_number(query)
        if case_number:
            logger.log(f"üß† –û–ø—Ä–µ–¥–µ–ª–µ–Ω –ø–æ–∏—Å–∫ –ø–æ –Ω–æ–º–µ—Ä—É –¥–µ–ª–∞: {case_number}", LogLevel.INFO)
            results = self.search_by_case_number(case_number, limit)
            return {
                "type": "case_number",
                "query_entity": case_number,
                "results": results
            }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ
        company = self.extract_company_name(query)
        if company:
            logger.log(f"üß† –û–ø—Ä–µ–¥–µ–ª–µ–Ω –ø–æ–∏—Å–∫ –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏: {company}", LogLevel.INFO)
            results = self.search_by_company(company, limit)
            return {
                "type": "company",
                "query_entity": company,
                "results": results
            }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–∏–ø–∞ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        doc_type = self.extract_document_type(query)
        if doc_type:
            logger.log(f"üß† –û–ø—Ä–µ–¥–µ–ª–µ–Ω –ø–æ–∏—Å–∫ –ø–æ —Ç–∏–ø—É –¥–æ–∫—É–º–µ–Ω—Ç–∞: {doc_type}", LogLevel.INFO)
            results = search_procedural_forms(query, min(limit, 5))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 —Ñ–æ—Ä–º

            if results:
                return {
                    "type": "procedural_form",
                    "query_entity": doc_type,
                    "results": results
                }

        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—â–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É –∑–∞–ø—Ä–æ—Å–∞
        logger.log(f"üß† –û–ø—Ä–µ–¥–µ–ª–µ–Ω –ø–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É –∑–∞–ø—Ä–æ—Å–∞", LogLevel.INFO)
        results = self.search_by_text_fragment(query, limit)
        return {
            "type": "text_fragment",
            "query_entity": query,
            "results": results
        }


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
_smart_search_service = None

def get_smart_search_service():
    """–°–∏–Ω–≥–ª—Ç–æ–Ω –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–µ—Ä–≤–∏—Å—É —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    global _smart_search_service
    if _smart_search_service is None:
        _smart_search_service = SmartSearchService(get_es_client())
    return _smart_search_service


# –ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–º–µ—Ä–æ–≤ –¥–µ–ª –≤–Ω–µ –∫–ª–∞—Å—Å–∞ SmartSearchService
def extract_case_numbers_from_query(query: str) -> List[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä–∞ –¥–µ–ª –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤.

    Args:
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –Ω–æ–º–µ—Ä–æ–≤ –¥–µ–ª
    """
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–º–µ—Ä–æ–≤ –¥–µ–ª —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
    pattern = r'[–êA]\d{1,2}-\d+/\d{2,4}(?:-[–ê-–Ø–∞-—èA-Za-z0-9]+)*'

    # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–µ
    if isinstance(query, bytes):
        query = query.decode('utf-8')

    match = re.search(pattern, query)

    if not match:
        logger.log(f"–ù–æ–º–µ—Ä –¥–µ–ª–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∑–∞–ø—Ä–æ—Å–µ: '{query}'", LogLevel.INFO)
        return []

    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π –Ω–æ–º–µ—Ä –¥–µ–ª–∞
    case_number = match.group(0)
    logger.log(f"–ò–∑–≤–ª–µ—á–µ–Ω –Ω–æ–º–µ—Ä –¥–µ–ª–∞: {case_number}", LogLevel.INFO)

    variants = [case_number]

    # –°–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –±—É–∫–≤–∞–º–∏ –ê/A
    if case_number.startswith("–ê"):  # —Ä—É—Å—Å–∫–∞—è –ê
        latin_variant = "A" + case_number[1:]
        variants.append(latin_variant)
        logger.log(f"–°–æ–∑–¥–∞–Ω –ª–∞—Ç–∏–Ω—Å–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç: {latin_variant}", LogLevel.INFO)
    elif case_number.startswith("A"):  # –ª–∞—Ç–∏–Ω—Å–∫–∞—è A
        russian_variant = "–ê" + case_number[1:]
        variants.append(russian_variant)
        logger.log(f"–°–æ–∑–¥–∞–Ω —Ä—É—Å—Å–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç: {russian_variant}", LogLevel.INFO)

    # –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    logger.log(f"–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω—ã –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞: {variants}", LogLevel.INFO)
    return variants


def search_procedural_forms(query: str, limit: int = 5) -> List[str]:
    """
    –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ procedural_forms_index (–ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤).

    Args:
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        List[str]: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    try:
        es = get_es_client()
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        index_name = ES_INDICES.get("procedural_forms", "procedural_forms_index")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        if not es.indices.exists(index=index_name):
            logger.log(f"üîç –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç", LogLevel.WARNING)
            return []

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ limit
        size = max(1, limit)  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ size –±—É–¥–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        smart_search = get_smart_search_service()
        doc_type = smart_search.extract_document_type(query)

        # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        should_clauses = [
            {"match": {"full_text": {"query": query, "boost": 1.0}}},
            {"match": {"title": {"query": query, "boost": 3.0}}},
            {"match": {"subject_matter": {"query": query, "boost": 2.0}}},
            {"match": {"category": {"query": query, "boost": 1.5}}},
            {"match": {"subcategory": {"query": query, "boost": 1.5}}}
        ]

        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –≤ –∑–∞–ø—Ä–æ—Å
        if doc_type:
            should_clauses.append({"match": {"doc_type": {"query": doc_type, "boost": 4.0}}})
            logger.log(f"üîç –û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {doc_type}", LogLevel.INFO)

        body = {
            "size": size,
            "query": {
                "bool": {
                    "should": should_clauses,
                    "minimum_should_match": 1
                }
            },
            "highlight": {
                "fields": {
                    "full_text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "title": {"pre_tags": ["<b>"], "post_tags": ["</b>"]}
                }
            }
        }

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        response = es.search(index=index_name, body=body)
        hits = response["hits"]["hits"]

        results = []
        for hit in hits:
            source = hit["_source"]

            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
            title = source.get("title", "")
            doc_type = source.get("doc_type", "")
            category = source.get("category", "")
            subcategory = source.get("subcategory", "")
            jurisdiction = source.get("jurisdiction", "")
            stage = source.get("stage", "")
            subject_matter = source.get("subject_matter", "")
            full_text = source.get("full_text", "")
            template_vars = source.get("template_variables", {})

            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥—Å–≤–µ—á–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            highlights = []
            for field in ["title", "full_text"]:
                if hit.get("highlight", {}).get(field):
                    highlights.extend(hit["highlight"][field])

            highlight_text = "...\n".join(highlights) if highlights else ""

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = f"–ü–†–û–¶–ï–°–°–£–ê–õ–¨–ù–ê–Ø –§–û–†–ú–ê: {title}\n"
            result += f"–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {doc_type}\n"

            if jurisdiction:
                result += f"–Æ—Ä–∏—Å–¥–∏–∫—Ü–∏—è: {jurisdiction}\n"

            if category:
                result += f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}\n"

            if subcategory:
                result += f"–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è: {subcategory}\n"

            if stage:
                result += f"–°—Ç–∞–¥–∏—è: {stage}\n"

            if subject_matter:
                result += f"–ü—Ä–µ–¥–º–µ—Ç: {subject_matter}\n"

            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —à–∞–±–ª–æ–Ω–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            if template_vars and isinstance(template_vars, dict) and template_vars:
                result += f"\n–®–∞–±–ª–æ–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è:\n"
                for key, value in template_vars.items():
                    if isinstance(value, list):
                        result += f"‚Ä¢ {key}: [{', '.join(value)}]\n"
                    else:
                        result += f"‚Ä¢ {key}: {value}\n"

            if highlight_text:
                result += f"\n–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n{highlight_text}\n\n"

            result += f"\n–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n{full_text[:2000]}"
            if len(full_text) > 2000:
                result += "...[—Ç–µ–∫—Å—Ç —Å–æ–∫—Ä–∞—â–µ–Ω]"

            results.append(result)

        logger.log(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", LogLevel.INFO)
        return results

    except Exception as e:
        logger.log(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ procedural_forms_index: {str(e)}", LogLevel.ERROR)
        logger.exception("–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ—à–∏–±–∫–µ:")
        return []


async def get_query_embedding(query: str) -> List[float]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç sentence-transformers —á–µ—Ä–µ–∑ EmbeddingService.
    
    Args:
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        
    Returns:
        List[float]: –í–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 384
    """
    try:
        embedding_service = EmbeddingService()
        return await embedding_service.get_embedding_async(query)
        
    except Exception as e:
        logger.log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {str(e)}", LogLevel.ERROR)
        return [0.0] * 384  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏

async def search_law_chunks(query: str, size: int = 5, use_vector: bool = True) -> List[Dict[str, Any]]:
    logger.search("ElasticSearch", query, context={"query": query, "size": size})
    try:
        es = get_es_client()
        search_query = {
            "query": {
                "bool": {
                    "should": [
                        {
                            "multi_match": {
                                "query": query,
                                "fields": [
                                    "text^3",
                                    "title^2", 
                                    "content^2",
                                    "full_text^2"
                                ],
                                "type": "best_fields",
                                "fuzziness": "AUTO"
                            }
                        },
                        {
                            "match_phrase": {
                                "text": {
                                    "query": query,
                                    "boost": 2
                                }
                            }
                        }
                    ],
                    "minimum_should_match": 1
                }
            },
            "highlight": {
                "fields": {
                    "text": {"fragment_size": 150, "number_of_fragments": 3}
                }
            },
            "size": size
        }
        logger.log(f"[ES] –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å: {json.dumps(search_query)[:300]}...", LogLevel.DEBUG)
        response = es.search(
            index="court_decisions_index",
            body=search_query
        )
        hits = response['hits']['hits']
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(hits)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ Elasticsearch", context={"query": query, "results_count": len(hits)})
        if hits:
            logger.log(f"[ES] –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {json.dumps(hits[0], ensure_ascii=False)[:500]}...", LogLevel.DEBUG)
        else:
            logger.log(f"[ES] –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}'", LogLevel.WARNING)
        results = []
        for hit in hits:
            result = {
                "text": hit["_source"].get("text", ""),
                "title": hit["_source"].get("title", ""),
                "score": hit["_score"],
                "highlights": hit.get("highlight", {}).get("text", [])
            }
            results.append(result)
        return results
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}", context={"query": query, "error": str(e)})
        return []


def search_court_decisions(es, query: str, limit: int) -> List[str]:
    """
    –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ court_decisions_index (—Å—É–¥–µ–±–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è).

    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        List[str]: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ü–†–ê–í–ò–õ–¨–ù–û–ï –∏–º—è –∏–Ω–¥–µ–∫—Å–∞ –Ω–∞–ø—Ä—è–º—É—é
        index_name = "court_decisions_index"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if not es.indices.exists(index=index_name):
            logger.log(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.", LogLevel.WARNING)
            return []

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –¥–µ–ª–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
        case_number_pattern = r'[–êA]\d{1,2}-\d+/\d{2,4}(?:-[–ê-–Ø–∞-—èA-Za-z0-9]+)*'
        case_number_matches = re.findall(case_number_pattern, query)

        case_numbers = []
        for number in case_number_matches:
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –Ω–æ–º–µ—Ä
            case_numbers.append(number)
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç —Å –¥—Ä—É–≥–æ–π –±—É–∫–≤–æ–π (–ê/A)
            if '–ê' in number:
                case_numbers.append(number.replace('–ê', 'A'))
            else:
                case_numbers.append(number.replace('A', '–ê'))

        if case_numbers:
            logger.log(f"üîç –ò–∑–≤–ª–µ—á–µ–Ω—ã –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞: {case_numbers}", LogLevel.INFO)

            # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞
            should_clauses = []
            for case_number in case_numbers:
                should_clauses.append({
                    "term": {
                        "case_number": case_number
                    }
                })
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∞–∫–∂–µ –ø–æ–∏—Å–∫ –ø–æ –ø–æ–ª–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É –¥–ª—è –Ω–æ–º–µ—Ä–æ–≤ –¥–µ–ª
                should_clauses.append({
                    "match_phrase": {
                        "full_text": case_number
                    }
                })

            body = {
                "size": limit,
                "query": {
                    "bool": {
                        "should": should_clauses,
                        "minimum_should_match": 1
                    }
                }
            }
            logger.log(f"üîç –ü–æ–∏—Å–∫ –ø–æ –Ω–æ–º–µ—Ä—É –¥–µ–ª–∞: {json.dumps(should_clauses[:2], ensure_ascii=False)}", LogLevel.INFO)
        else:
            # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π multi_match
            body = {
                "size": limit,
                "query": {
                    "multi_match": {
                        "query": query,
                        "fields": [
                            "case_number^5",
                            "judges^3",
                            "claimant^3",
                            "defendant^3",
                            "subject^2",
                            "arguments",
                            "conclusion",
                            "full_text"
                        ],
                        "type": "best_fields",
                        "fuzziness": "AUTO"
                    }
                }
            }

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Å–≤–µ—Ç–∫—É –¥–ª—è –ª—é–±–æ–≥–æ —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞
        body["highlight"] = {
            "fields": {
                "full_text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]}
            },
            "fragment_size": 300,
            "number_of_fragments": 3
        }

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        logger.log(f"üîç –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫: {json.dumps(body, ensure_ascii=False)[:200]}...", LogLevel.INFO)
        response = es.search(index=index_name, body=body)
        hits = response["hits"]["hits"]

        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        if hits:
            found_case_numbers = [hit["_source"].get("case_number", "N/A") for hit in hits[:3]]
            logger.log(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(hits)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ü–µ—Ä–≤—ã–µ –Ω–æ–º–µ—Ä–∞ –¥–µ–ª: {', '.join(found_case_numbers)}", LogLevel.INFO)
        else:
            logger.log(f"üîç –ü–æ –∑–∞–ø—Ä–æ—Å—É '{query}' —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ", LogLevel.WARNING)

        results = []
        for hit in hits:
            source = hit["_source"]
            case_number = source.get("case_number", "")
            subject = source.get("subject", "")
            judges = source.get("judges", "")
            claimant = source.get("claimant", "")
            defendant = source.get("defendant", "")
            full_text = source.get("full_text", "")

            highlights = hit.get("highlight", {}).get("full_text", [])
            highlight_text = "...\n".join(highlights)

            result = f"–°—É–¥–µ–±–Ω–æ–µ –¥–µ–ª–æ ‚Ññ {case_number}\n"
            result += f"–°—É–¥—å–∏: {judges}\n–ò—Å—Ç–µ—Ü: {claimant}\n–û—Ç–≤–µ—Ç—á–∏–∫: {defendant}\n–ü—Ä–µ–¥–º–µ—Ç: {subject}\n"

            if highlights:
                result += f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n{highlight_text}\n\n"

            result += f"–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç:\n{full_text[:2000]}..."

            results.append(result)

        return results

    except Exception as e:
        logger.log(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ court_decisions_index: {str(e)}", LogLevel.ERROR)
        logger.exception("–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ—à–∏–±–∫–µ:")
        return []


def search_ruslawod_chunks(es, query: str, limit: int) -> List[str]:
    """
    –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ ruslawod_chunks_index (—á–∞–Ω–∫–∏ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞).

    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        List[str]: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        index_name = ES_INDICES.get("ruslawod_chunks", "ruslawod_chunks_index")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if not es.indices.exists(index=index_name):
            logger.log(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.", LogLevel.WARNING)
            return []

        # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        body = {
            "size": limit,
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": [
                        "text^3",
                        "content^3",
                        "full_text^2",
                        "law_name",
                        "section_name",
                        "article_name"
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            },
            "highlight": {
                "fields": {
                    "text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "content": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "full_text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]}
                },
                "fragment_size": 300,
                "number_of_fragments": 3
            }
        }

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        response = es.search(index=index_name, body=body)
        hits = response["hits"]["hits"]

        results = []
        for hit in hits:
            source = hit["_source"]

            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
            law_name = source.get("law_name", "")
            section = source.get("section_name", source.get("section", ""))
            article = source.get("article_name", source.get("article", ""))
            content = source.get("content", source.get("text", source.get("full_text", "")))

            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
            document_id = source.get("document_id", "")
            chunk_id = source.get("chunk_id", "")

            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥—Å–≤–µ—á–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            highlights = []
            for field in ["text", "content", "full_text"]:
                if hit.get("highlight", {}).get(field):
                    highlights.extend(hit["highlight"][field])

            highlight_text = "...\n".join(highlights) if highlights else ""

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = f"–ó–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ: {law_name}\n"

            if section:
                result += f"–†–∞–∑–¥–µ–ª: {section}\n"

            if article:
                result += f"–°—Ç–∞—Ç—å—è: {article}\n"

            if document_id and chunk_id:
                result += f"ID –¥–æ–∫—É–º–µ–Ω—Ç–∞: {document_id}, ID —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞: {chunk_id}\n"

            if highlight_text:
                result += f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n{highlight_text}\n\n"

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            if content:
                content_preview = content[:2000] + "..." if len(content) > 2000 else content
                result += f"–¢–µ–∫—Å—Ç:\n{content_preview}"

            results.append(result)

        logger.log(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞", LogLevel.INFO)
        return results

    except Exception as e:
        logger.log(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ ruslawod_chunks_index: {str(e)}", LogLevel.ERROR)
        return []

def search_court_reviews(es, query: str, limit: int) -> List[str]:
    """
    –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ court_reviews_index (–æ–±–∑–æ—Ä—ã —Å—É–¥–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π).

    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        List[str]: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        index_name = ES_INDICES.get("court_reviews", "court_reviews_index")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if not es.indices.exists(index=index_name):
            logger.log(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.", LogLevel.WARNING)
            return []

        # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        body = {
            "size": limit,
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": [
                        "title^4",
                        "content^3",
                        "description^2",
                        "text",
                        "full_text"
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            },
            "highlight": {
                "fields": {
                    "title": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "content": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "full_text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]}
                },
                "fragment_size": 300,
                "number_of_fragments": 3
            }
        }

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        response = es.search(index=index_name, body=body)
        hits = response["hits"]["hits"]

        results = []
        for hit in hits:
            source = hit["_source"]

            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
            title = source.get("title", "–û–±–∑–æ—Ä —Å—É–¥–µ–±–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏")
            content = source.get("content", source.get("text", source.get("full_text", "")))
            author = source.get("author", "")
            publication_date = source.get("publication_date", source.get("date", ""))
            source_name = source.get("source", "")

            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥—Å–≤–µ—á–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            highlights = []
            for field in ["title", "content", "text", "full_text"]:
                if hit.get("highlight", {}).get(field):
                    highlights.extend(hit["highlight"][field])

            highlight_text = "...\n".join(highlights) if highlights else ""

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = f"–û–±–∑–æ—Ä —Å—É–¥–µ–±–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏: {title}\n"

            if author:
                result += f"–ê–≤—Ç–æ—Ä: {author}\n"

            if publication_date:
                result += f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {publication_date}\n"

            if source_name:
                result += f"–ò—Å—Ç–æ—á–Ω–∏–∫: {source_name}\n"

            if highlight_text:
                result += f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n{highlight_text}\n\n"

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            if content:
                content_preview = content[:2000] + "..." if len(content) > 2000 else content
                result += f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n{content_preview}"

            results.append(result)

        logger.log(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} –æ–±–∑–æ—Ä–æ–≤ —Å—É–¥–µ–±–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏", LogLevel.INFO)
        return results

    except Exception as e:
        logger.log(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ court_reviews_index: {str(e)}", LogLevel.ERROR)
        return []

def search_legal_articles(es, query: str, limit: int) -> List[str]:
    """
    –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ legal_articles_index (–ø—Ä–∞–≤–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏).

    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        List[str]: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        index_name = ES_INDICES.get("legal_articles", "legal_articles_index")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if not es.indices.exists(index=index_name):
            logger.log(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.", LogLevel.WARNING)
            return []

        # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        body = {
            "size": limit,
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": [
                        "title^4",
                        "content^3",
                        "description^2",
                        "body",
                        "text"
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            },
            "highlight": {
                "fields": {
                    "title": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "content": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "body": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]}
                },
                "fragment_size": 300,
                "number_of_fragments": 3
            }
        }

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        response = es.search(index=index_name, body=body)
        hits = response["hits"]["hits"]

        results = []
        for hit in hits:
            source = hit["_source"]

            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
            title = source.get("title", "–ü—Ä–∞–≤–æ–≤–∞—è —Å—Ç–∞—Ç—å—è")
            content = source.get("content", source.get("body", source.get("text", "")))
            author = source.get("author", "")
            publication_date = source.get("publication_date", source.get("date", ""))
            source_name = source.get("source", "")
            tags = source.get("tags", source.get("keywords", ""))
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥—Å–≤–µ—á–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            highlights = []
            for field in ["title", "content", "body", "text"]:
                if hit.get("highlight", {}).get(field):
                    highlights.extend(hit["highlight"][field])

            highlight_text = "...\n".join(highlights) if highlights else ""

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = f"–ü—Ä–∞–≤–æ–≤–∞—è —Å—Ç–∞—Ç—å—è: {title}\n"

            if author:
                result += f"–ê–≤—Ç–æ—Ä: {author}\n"

            if publication_date:
                result += f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {publication_date}\n"

            if source_name:
                result += f"–ò—Å—Ç–æ—á–Ω–∏–∫: {source_name}\n"

            if tags:
                tags_str = tags if isinstance(tags, str) else ", ".join(tags)
                result += f"–¢–µ–≥–∏: {tags_str}\n"

            if highlight_text:
                result += f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n{highlight_text}\n\n"

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            if content:
                content_preview = content[:2000] + "..." if len(content) > 2000 else content
                result += f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n{content_preview}"

            results.append(result)

        logger.log(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} –ø—Ä–∞–≤–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π", LogLevel.INFO)
        return results

    except Exception as e:
        logger.log(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ legal_articles_index: {str(e)}", LogLevel.ERROR)
        return []

def create_court_decisions_index(es):
    """
    –°–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å court_decisions –≤ Elasticsearch, –µ—Å–ª–∏ –æ–Ω –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.

    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
    """
    index_name = ES_INDICES.get("court_decisions", "court_decisions_index")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
    if not es.indices.exists(index=index_name):
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞
        mappings = {
            "properties": {
                "case_number": {"type": "text", "analyzer": "russian"},
                "judges": {"type": "text", "analyzer": "russian"},
                "claimant": {"type": "text", "analyzer": "russian"},
                "defendant": {"type": "text", "analyzer": "russian"},
                "subject": {"type": "text", "analyzer": "russian"},
                "arguments": {"type": "text", "analyzer": "russian"},
                "conclusion": {"type": "text", "analyzer": "russian"},
                "full_text": {"type": "text", "analyzer": "russian"}
            }
        }

        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
        es.indices.create(
            index=index_name,
            body={
                "settings": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "analysis": {
                        "analyzer": {
                            "russian": {
                                "tokenizer": "standard",
                                "filter": ["lowercase", "russian_morphology", "russian_stop"]
                            }
                        }
                    }
                },
                "mappings": mappings
            }
        )

        logger.log(f"‚úÖ –ò–Ω–¥–µ–∫—Å {index_name} —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω.", LogLevel.INFO)
    else:
        logger.log(f"‚úÖ –ò–Ω–¥–µ–∫—Å {index_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.", LogLevel.INFO)

def create_ruslawod_chunks_index(es):
    """
    –°–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å ruslawod_chunks_index –≤ Elasticsearch, –µ—Å–ª–∏ –æ–Ω –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.

    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
    """
    index_name = ES_INDICES.get("ruslawod_chunks", "ruslawod_chunks_index")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
    if not es.indices.exists(index=index_name):
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞
        mappings = {
            "properties": {
                "law_name": {"type": "text", "analyzer": "russian"},
                "section_name": {"type": "text", "analyzer": "russian"},
                "article_name": {"type": "text", "analyzer": "russian"},
                "document_id": {"type": "keyword"},
                "chunk_id": {"type": "keyword"},
                "content": {"type": "text", "analyzer": "russian"},
                "text": {"type": "text", "analyzer": "russian"},
                "full_text": {"type": "text", "analyzer": "russian"}
            }
        }

        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
        es.indices.create(
            index=index_name,
            body={
                "settings": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "analysis": {
                        "analyzer": {
                            "russian": {
                                "tokenizer": "standard",
                                "filter": ["lowercase", "russian_morphology", "russian_stop"]
                            }
                        }
                    }
                },
                "mappings": mappings
            }
        )

        logger.log(f"‚úÖ –ò–Ω–¥–µ–∫—Å {index_name} —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω.", LogLevel.INFO)
    else:
        logger.log(f"‚úÖ –ò–Ω–¥–µ–∫—Å {index_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.", LogLevel.INFO)

def create_procedural_forms_index(es):
    """
    –°–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å procedural_forms_index –≤ Elasticsearch, –µ—Å–ª–∏ –æ–Ω –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.

    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
    """
    index_name = ES_INDICES.get("procedural_forms", "procedural_forms_index")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
    if not es.indices.exists(index=index_name):
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞
        mappings = {
            "properties": {
                "id": {"type": "integer"},
                "doc_id": {"type": "keyword"},
                "title": {"type": "text", "analyzer": "russian", 
                        "fields": {"keyword": {"type": "keyword"}}},
                "doc_type": {"type": "keyword"},
                "court_type": {"type": "keyword"},
                "target_court": {"type": "text", "analyzer": "russian"},
                "jurisdiction": {"type": "keyword"},
                "category": {"type": "keyword"},
                "subcategory": {"type": "keyword"},
                "applicant_type": {"type": "keyword"},
                "respondent_type": {"type": "keyword"},
                "third_parties": {"type": "keyword"},  # –ú–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫
                "stage": {"type": "keyword"},
                "subject_matter": {"type": "text", "analyzer": "russian"},
                "keywords": {"type": "keyword"},  # –ú–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫
                "legal_basis": {"type": "keyword"},  # –ú–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫
                "full_text": {"type": "text", "analyzer": "russian"},
                "template_variables": {"type": "object"},  # JSONB
                "source_file": {"type": "keyword"},
                "creation_date": {"type": "date"},
                "last_updated": {"type": "date"},
                # –ü–æ–ª—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                "content": {"type": "text", "analyzer": "russian", "copy_to": "full_text"},
                "text": {"type": "text", "analyzer": "russian", "copy_to": "full_text"},
                "indexed_at": {"type": "date"}
            }
        }

        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
        es.indices.create(
            index=index_name,
            body={
                "settings": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "analysis": {
                        "analyzer": {
                            "russian": {
                                "tokenizer": "standard",
                                "filter": ["lowercase", "russian_morphology", "russian_stop"]
                            }
                        }
                    }
                },
                "mappings": mappings
            }
        )

        logger.log(f"‚úÖ –ò–Ω–¥–µ–∫—Å {index_name} —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω.", LogLevel.INFO)
    else:
        logger.log(f"‚úÖ –ò–Ω–¥–µ–∫—Å {index_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.", LogLevel.INFO)

def create_indices():
    """
    –°–æ–∑–¥–∞–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–Ω–¥–µ–∫—Å—ã –≤ Elasticsearch.
    """
    try:
        es = get_es_client()

        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
        create_court_decisions_index(es)
        create_ruslawod_chunks_index(es)
        create_procedural_forms_index(es)  # –î–æ–±–∞–≤–ª–µ–Ω–æ —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º

        # –¢–∞–∫–∂–µ –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è court_reviews_index –∏ legal_articles_index
        # –Ω–æ –ø–æ–∫–∞ –æ—Å—Ç–∞–≤–∏–º –∏—Ö –±–µ–∑ —è–≤–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è

        logger.log("‚úÖ –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–Ω–¥–µ–∫—Å—ã –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –∏–ª–∏ —Å–æ–∑–¥–∞–Ω—ã.", LogLevel.INFO)

    except Exception as e:
        logger.log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–æ–≤: {str(e)}", LogLevel.ERROR)

def index_court_decisions():
    """
    –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å—É–¥–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –≤ Elasticsearch.
    –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π.
    """
    create_indices()

def update_court_decisions_mapping(es=None):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ —Å—É–¥–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π, –¥–æ–±–∞–≤–ª—è—è –ø–æ–¥–¥–µ—Ä–∂–∫—É
    –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –Ω–æ–º–µ—Ä—É –¥–µ–ª–∞ –∏ –∫–æ–º–ø–∞–Ω–∏–∏.

    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)    """
    if es is None:
        es = get_es_client()

    try:
        index_name = ES_INDICES.get("court_decisions", "court_decisions_index")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if not es.indices.exists(index=index_name):
            logger.log(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.", LogLevel.WARNING)
            return False

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞
        mapping_update = {
            "properties": {
                "case_number": { 
                    "type": "keyword",
                    "fields": {
                        "text": { "type": "text", "analyzer": "russian" }
                    }
                },
                "defendant": { 
                    "type": "text", 
                    "analyzer": "russian",
                    "fields": {
                        "keyword": { "type": "keyword" }
                    }
                },
                "claimant": { 
                    "type": "text", 
                    "analyzer": "russian",
                    "fields": {
                        "keyword": { "type": "keyword" }
                    }
                },
                "doc_id": { "type": "keyword" },
                "chunk_id": { "type": "integer" }
            }
        }

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥
        es.indices.put_mapping(
            index=index_name,
            body=mapping_update
        )

        logger.log(f"‚úÖ –ú–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {index_name} —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω.", LogLevel.INFO)
        return True
    except Exception as e:
        logger.log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –º–∞–ø–ø–∏–Ω–≥–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {index_name}: {str(e)}", LogLevel.ERROR)
        return False

def update_procedural_forms_mapping(es=None):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º, –¥–æ–±–∞–≤–ª—è—è –ø–æ–¥–¥–µ—Ä–∂–∫—É
    –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.

    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    if es is None:
        es = get_es_client()

    try:
        index_name = ES_INDICES.get("procedural_forms", "procedural_forms_index")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if not es.indices.exists(index=index_name):
            logger.log(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.", LogLevel.WARNING)
            return False

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞
        mapping_update = {
            "properties": {
                "title": { 
                    "type": "text", 
                    "analyzer": "russian",
                    "fields": {
                        "keyword": { "type": "keyword" }
                    }
                },
                "doc_type": { "type": "keyword" },
                "category": { "type": "keyword" },
                "subcategory": { "type": "keyword" },
                "subject_matter": { 
                    "type": "text", 
                    "analyzer": "russian" 
                },
                "keywords": { "type": "keyword" },
                "doc_id": { "type": "keyword" }
            }
        }

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥
        es.indices.put_mapping(
            index=index_name,
            body=mapping_update
        )

        logger.log(f"‚úÖ –ú–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {index_name} —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω.", LogLevel.INFO)
        return True
    except Exception as e:
        logger.log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –º–∞–ø–ø–∏–Ω–≥–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {index_name}: {str(e)}", LogLevel.ERROR)
        return False

def update_all_mappings():
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –º–∞–ø–ø–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.
    """
    try:
        es = get_es_client()

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
        success1 = update_court_decisions_mapping(es)
        success2 = update_procedural_forms_mapping(es)

        # –î–æ–±–∞–≤—å—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥—Ä—É–≥–∏—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏

        return success1 and success2
    except Exception as e:
        logger.log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –º–∞–ø–ø–∏–Ω–≥–æ–≤: {str(e)}", LogLevel.ERROR)
        return False

def search_law_chunks_multi(queries: list[str], size: int = 5) -> list[dict]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∑–∞–ø—Ä–æ—Å–∞–º –≤ Elasticsearch –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
    Args:
        queries: –°–ø–∏—Å–æ–∫ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        size: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å
    Returns:
        –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    all_results = []
    seen = set()
    for q in queries:
        results = search_law_chunks(q, size)
        for r in results:
            # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ —Ç–µ–∫—Å—Ç—É –∏ –∑–∞–≥–æ–ª–æ–≤–∫—É
            key = (r.get("text", "")[:100], r.get("title", ""))
            if key not in seen:
                seen.add(key)
                all_results.append(r)
    return all_results

# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–ª–∏—á–∏—è –ø–æ–ª—è embedding –≤ –∏–Ω–¥–µ–∫—Å–µ
async def has_embedding_field(es, index_name: str) -> bool:
    try:
        mapping = es.indices.get_mapping(index=index_name)
        props = mapping[index_name]['mappings'].get('properties', {})
        return 'embedding' in props
    except Exception:
        return False

# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º –¥–ª—è –ª—é–±–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
async def search_index_with_embeddings(index_name: str, query: str, size: int = 5, use_vector: bool = True) -> list:
    es = get_es_client()
    if not es.indices.exists(index=index_name):
        logger.log(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.", LogLevel.WARNING)
        return []
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–æ–ª—è embedding
    has_embedding = await has_embedding_field(es, index_name)
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    text_query = {
        "bool": {
            "should": [
                {"multi_match": {
                    "query": query,
                    "fields": ["title^3", "text^2", "content^2", "full_text^2", "subject_matter^2", "category", "subcategory", "body", "description", "keywords"],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }}
            ],
            "minimum_should_match": 1
        }
    }
    search_query = {"query": text_query}
    if use_vector and has_embedding:
        query_vector = await get_query_embedding(query)
        search_query = {
            "knn": {
                "field": "embedding",
                "query_vector": query_vector,
                "k": size,
                "num_candidates": size * 2
            },
            "rank_score": 0.4,
            "query": {
                "script_score": {
                    "query": text_query,
                    "script": {"source": "_score * 0.6"}
                }
            }
        }
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Å–≤–µ—Ç–∫—É
    search_query["highlight"] = {
        "fields": {
            "text": {"fragment_size": 150, "number_of_fragments": 3},
            "title": {"fragment_size": 150, "number_of_fragments": 2},
            "full_text": {"fragment_size": 150, "number_of_fragments": 2},
            "content": {"fragment_size": 150, "number_of_fragments": 2}
        }
    }
    try:
        response = es.search(index=index_name, body=search_query, size=size)
        hits = response["hits"]["hits"]
        results = []
        for hit in hits:
            source = hit["_source"]
            result = {
                "title": source.get("title", ""),
                "text": source.get("text", source.get("content", source.get("full_text", ""))),
                "score": hit.get("_score"),
                "vector_score": hit.get("_vector_score"),
                "highlights": sum([hit.get("highlight", {}).get(f, []) for f in ["text", "title", "full_text", "content"]], [])
            }
            results.append(result)
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –∏–Ω–¥–µ–∫—Å—É {index_name}", context={"query": query, "results_count": len(results)})
        return results
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∏–Ω–¥–µ–∫—Å—É {index_name}: {str(e)}", context={"query": query, "error": str(e)})
        return []

# –û–±—ë—Ä—Ç–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
async def search_procedural_forms_with_embeddings(query: str, size: int = 5, use_vector: bool = True):
    index_name = ES_INDICES.get("procedural_forms", "procedural_forms_index")
    return await search_index_with_embeddings(index_name, query, size, use_vector)

async def search_court_reviews_with_embeddings(query: str, size: int = 5, use_vector: bool = True):
    index_name = ES_INDICES.get("court_reviews", "court_reviews_index")
    return await search_index_with_embeddings(index_name, query, size, use_vector)

async def search_legal_articles_with_embeddings(query: str, size: int = 5, use_vector: bool = True):
    index_name = ES_INDICES.get("legal_articles", "legal_articles_index")
    return await search_index_with_embeddings(index_name, query, size, use_vector)

async def search_ruslawod_chunks_with_embeddings(query: str, size: int = 5, use_vector: bool = True):
    index_name = ES_INDICES.get("ruslawod_chunks", "ruslawod_chunks_index")
    return await search_index_with_embeddings(index_name, query, size, use_vector)

async def search_court_decisions_with_embeddings(query: str, size: int = 5, use_vector: bool = True):
    index_name = ES_INDICES.get("court_decisions", "court_decisions_index")
    return await search_index_with_embeddings(index_name, query, size, use_vector)

if __name__ == "__main__":
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –º–æ–¥—É–ª—è –Ω–∞–ø—Ä—è–º—É—é
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ Elasticsearch")
    create_indices()

    # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥–∏ –¥–ª—è —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    logger.info("–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–æ–≤ –¥–ª—è —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
    update_all_mappings()

    # –ü—Ä–∏–º–µ—Ä –ø–æ–∏—Å–∫–∞
    test_query = "–ê65-28469/2012"
    logger.info(f"–¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {test_query}")
    results = search_law_chunks(test_query)
    
    if results:
        logger.info("–ü–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:", context={"result": results[0]['text'][:500]})