# app/handlers/es_law_search.py

import logging
from typing import List, Dict, Any, Optional
from elasticsearch import Elasticsearch
import re
import json
from app.config import ELASTICSEARCH_URL, ES_USER, ES_PASS, ES_INDICES as CONFIG_ES_INDICES

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# –ò–Ω–¥–µ–∫—Å—ã –≤ Elasticsearch —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
DEFAULT_ES_INDICES = {
    "court_decisions": "court_decisions_index",
    "court_reviews": "court_reviews_index",
    "legal_articles": "legal_articles_index",
    "ruslawod_chunks": "ruslawod_chunks_index"
}

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
ES_INDICES = CONFIG_ES_INDICES or DEFAULT_ES_INDICES

def get_es_client():
    """
    –°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç Elasticsearch.
    
    Returns:
        Elasticsearch: –ö–ª–∏–µ–Ω—Ç Elasticsearch
    """
    try:
        es = Elasticsearch(
            ELASTICSEARCH_URL,
            basic_auth=(ES_USER, ES_PASS),
            retry_on_timeout=True,
            max_retries=3
        )
        return es
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Elasticsearch: {e}")
        raise

# –£–º–Ω—ã–π –ø–æ–∏—Å–∫ - –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
class SmartSearchService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ Elasticsearch"""
    
    def __init__(self, es_client):
        self.es = es_client
        self.court_decisions_index = ES_INDICES.get("court_decisions", "court_decisions_index")
        self.court_reviews_index = ES_INDICES.get("court_reviews", "court_reviews_index")
        self.legal_articles_index = ES_INDICES.get("legal_articles", "legal_articles_index")
        self.ruslawod_chunks_index = ES_INDICES.get("ruslawod_chunks", "ruslawod_chunks_index")
    

    def extract_case_number(self, query: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä –¥–µ–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–º–µ—Ä–æ–≤ –¥–µ–ª —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
        pattern = r'[–êA]\d{1,2}-\d+/\d{2,4}(?:-[–ê-–Ø–∞-—èA-Za-z0-9]+)*'
        
        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–µ
        if isinstance(query, bytes):
            query = query.decode('utf-8')
        
        match = re.search(pattern, query)
        if match:
            case_number = match.group(0)
            logger.info(f"SmartSearchService: –ò–∑–≤–ª–µ—á–µ–Ω –Ω–æ–º–µ—Ä –¥–µ–ª–∞: {case_number}")
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≥–æ–¥, –µ—Å–ª–∏ –æ–Ω –≤ –∫–æ—Ä–æ—Ç–∫–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 05 -> 2005)
            parts = case_number.split('/')
            if len(parts) > 1:
                year_part = parts[1].split('-')[0]  # –±–µ—Ä–µ–º –≥–æ–¥ –¥–æ –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ —Å—É—Ñ—Ñ–∏–∫—Å–∞
                if len(year_part) == 2:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º 2-–∑–Ω–∞—á–Ω—ã–π –≥–æ–¥ –≤ 4-–∑–Ω–∞—á–Ω—ã–π (05 -> 2005)
                    full_year = f"20{year_part}" if int(year_part) < 50 else f"19{year_part}"
                    case_number = case_number.replace(f"/{year_part}", f"/{full_year}", 1)
                    logger.info(f"SmartSearchService: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω –≥–æ–¥ –≤ –Ω–æ–º–µ—Ä–µ –¥–µ–ª–∞: {case_number}")
            
            return case_number
        logger.info(f"SmartSearchService: –ù–æ–º–µ—Ä –¥–µ–ª–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∑–∞–ø—Ä–æ—Å–µ: '{query}'")
        return None
    

    def extract_company_name(self, query: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        # –ü–æ–∏—Å–∫ –ø–æ —à–∞–±–ª–æ–Ω–∞–º "–û–û–û", "–ó–ê–û", "–û–ê–û", "–ü–ê–û" –∏ —Ç.–¥.
        patterns = [
            r'–û–û–û\s+[¬´"]?([^¬ª"]+)[¬ª"]?',
            r'–ó–ê–û\s+[¬´"]?([^¬ª"]+)[¬ª"]?',
            r'–û–ê–û\s+[¬´"]?([^¬ª"]+)[¬ª"]?',
            r'–ü–ê–û\s+[¬´"]?([^¬ª"]+)[¬ª"]?',
            r'–ò–ü\s+([–ê-–Ø–∞-—è\s]+)',
            r'–û–ì–†–ù\s+(\d{13}|\d{15})',
            r'–ò–ù–ù\s+(\d{10}|\d{12})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, query)
            if match:
                return match.group(0)
        return None
    
    def search_by_case_number(self, case_number: str, limit: int = 10) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—É–¥–µ–±–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –ø–æ –Ω–æ–º–µ—Ä—É –¥–µ–ª–∞"""
        logger.info(f"üîç –ü–æ–∏—Å–∫ –ø–æ –Ω–æ–º–µ—Ä—É –¥–µ–ª–∞: {case_number}")
        
        # –†–∞–∑–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä –¥–µ–ª–∞ –Ω–∞ —Å–æ—Å—Ç–∞–≤–Ω—ã–µ —á–∞—Å—Ç–∏
        full_case_number = case_number
        
        # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é —á–∞—Å—Ç—å –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞ (–±–µ–∑ —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤ –ø–æ—Å–ª–µ –≥–æ–¥–∞)
        base_parts = full_case_number.split('-')
        court_code = base_parts[0]  # A40 –∏–ª–∏ –ê40
        
        # –°–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –∞–ª—Ñ–∞–≤–∏—Ç–∞–º–∏
        variants = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π –Ω–æ–º–µ—Ä —Å –æ–±–æ–∏–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –±—É–∫–≤—ã –ê/A
        variants.append(full_case_number)
        if '–ê' in full_case_number:
            variants.append(full_case_number.replace('–ê', 'A'))
        else:
            variants.append(full_case_number.replace('A', '–ê'))
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—É—Ñ—Ñ–∏–∫—Å—ã –ø–æ—Å–ª–µ –≥–æ–¥–∞, –¥–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –±–µ–∑ –Ω–∏—Ö
        if len(base_parts) > 2:
            year_parts = base_parts[1].split('/')
            if len(year_parts) > 1:
                base_case_number = f"{court_code}-{base_parts[1]}"
                variants.append(base_case_number)
                if '–ê' in base_case_number:
                    variants.append(base_case_number.replace('–ê', 'A'))
                else:
                    variants.append(base_case_number.replace('A', '–ê'))
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        variants = list(set(variants))
        logger.info(f"üîç –í–∞—Ä–∏–∞–Ω—Ç—ã –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞: {variants}")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
        should_clauses = []
        for variant in variants:
            should_clauses.append({"term": {"case_number": variant}})
            should_clauses.append({"match_phrase": {"full_text": variant}})
        
        body = {
            "query": {
                "bool": {
                    "should": should_clauses,
                    "minimum_should_match": 1
                }
            },
            "size": limit,
            "sort": [
                {"doc_id": {"order": "asc"}},
                {"chunk_id": {"order": "asc"}}
            ]
        }
        
        try:
            # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            logger.info(f"üîç DEBUG: –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å: {json.dumps(body, ensure_ascii=False)}")
            
            response = self.es.search(index=self.court_decisions_index, body=body)
            hits = response["hits"]["hits"]
            
            if hits:
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                results = []
                doc_id_set = set()
                
                for hit in hits:
                    source = hit["_source"]
                    doc_id = source.get("doc_id", "")
                    doc_id_set.add(doc_id)
                    results.append(source)
                
                logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –¥–µ–ª–∞ {case_number} (–¥–æ–∫—É–º–µ–Ω—Ç—ã: {len(doc_id_set)})")
                return results
            else:
                logger.warning(f"üîç –î–µ–ª–æ {case_number} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                return []
                    
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–µ–ª–∞ {case_number}: {str(e)}")
            logger.exception("Stacktrace:")
            return []
    
    def search_by_company(self, company: str, limit: int = 10) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –¥–µ–ª —Å —É—á–∞—Å—Ç–∏–µ–º —É–∫–∞–∑–∞–Ω–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏"""
        logger.info(f"üîç –ü–æ–∏—Å–∫ –¥–µ–ª —Å —É—á–∞—Å—Ç–∏–µ–º –∫–æ–º–ø–∞–Ω–∏–∏: {company}")
        
        body = {
            "size": limit,
            "query": {
                "bool": {
                    "should": [
                        {"match": {"claimant": company}},
                        {"match": {"defendant": company}},
                        {"match": {"full_text": company}}
                    ],
                    "minimum_should_match": 1
                }
            },
            "collapse": {
                "field": "doc_id"  # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
            },
            "_source": ["doc_id", "case_number", "court_name", "date", "subject", "claimant", "defendant"]
        }
        
        try:
            response = self.es.search(index=self.court_decisions_index, body=body)
            hits = response["hits"]["hits"]
            
            if hits:
                results = [hit["_source"] for hit in hits]
                logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} –¥–µ–ª —Å —É—á–∞—Å—Ç–∏–µ–º {company}")
                return results
            else:
                logger.warning(f"üîç –î–µ–ª–∞ —Å —É—á–∞—Å—Ç–∏–µ–º {company} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return []
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∫–æ–º–ø–∞–Ω–∏–∏ {company}: {str(e)}")
            return []
    
    def search_by_text_fragment(self, text: str, limit: int = 10) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Ç–µ–∫—Å—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (–ø—Ä–µ–¥—ã–¥—É—â–∏–π –∏ —Å–ª–µ–¥—É—é—â–∏–π —á–∞–Ω–∫–∏)"""
        logger.info(f"üîç –ü–æ–∏—Å–∫ –ø–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—É —Ç–µ–∫—Å—Ç–∞: {text[:100]}...")
        
        body = {
            "size": limit,
            "query": {
                "match_phrase": {
                    "full_text": text
                }
            },
            "highlight": {
                "fields": {
                    "full_text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]}
                }
            }
        }
        
        try:
            response = self.es.search(index=self.court_decisions_index, body=body)
            hits = response["hits"]["hits"]
            
            if not hits:
                logger.warning(f"üîç –§—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return []
            
            results = []
            
            for hit in hits:
                source = hit["_source"]
                doc_id = source.get("doc_id", "")
                chunk_id = source.get("chunk_id", 0)
                
                # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –∏ —Å–ª–µ–¥—É—é—â–∏–π —á–∞–Ω–∫–∏
                prev_chunk = self._get_adjacent_chunk(doc_id, chunk_id - 1)
                next_chunk = self._get_adjacent_chunk(doc_id, chunk_id + 1)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                result = {
                    "current": source,
                    "highlight": hit.get("highlight", {}).get("full_text", []),
                    "prev": prev_chunk,
                    "next": next_chunk
                }
                
                results.append(result)
            
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º")
            return results
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
            return []
    
    def _get_adjacent_chunk(self, doc_id: str, chunk_id: int) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–º–µ–∂–Ω—ã–π —á–∞–Ω–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        if chunk_id < 1:
            return None
            
        body = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"doc_id": doc_id}},
                        {"term": {"chunk_id": chunk_id}}
                    ]
                }
            }
        }
        
        try:
            response = self.es.search(index=self.court_decisions_index, body=body)
            hits = response["hits"]["hits"]
            
            if hits:
                return hits[0]["_source"]
            return None
                
        except Exception:
            return None
    
    def smart_search(self, query: str, limit: int = 10) -> Dict[str, Any]:
        """–£–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –∏ –≤—ã–±–æ—Ä –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –º–µ—Ç–æ–¥–∞ –ø–æ–∏—Å–∫–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞ –≤ –∑–∞–ø—Ä–æ—Å–µ
        case_number = self.extract_case_number(query)
        if case_number:
            logger.info(f"üß† –û–ø—Ä–µ–¥–µ–ª–µ–Ω –ø–æ–∏—Å–∫ –ø–æ –Ω–æ–º–µ—Ä—É –¥–µ–ª–∞: {case_number}")
            results = self.search_by_case_number(case_number, limit)
            return {
                "type": "case_number",
                "query_entity": case_number,
                "results": results
            }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ
        company = self.extract_company_name(query)
        if company:
            logger.info(f"üß† –û–ø—Ä–µ–¥–µ–ª–µ–Ω –ø–æ–∏—Å–∫ –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏: {company}")
            results = self.search_by_company(company, limit)
            return {
                "type": "company",
                "query_entity": company,
                "results": results
            }
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—â–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É –∑–∞–ø—Ä–æ—Å–∞
        logger.info(f"üß† –û–ø—Ä–µ–¥–µ–ª–µ–Ω –ø–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É –∑–∞–ø—Ä–æ—Å–∞")
        results = self.search_by_text_fragment(query, limit)
        return {
            "type": "text_fragment",
            "query_entity": query,
            "results": results
        }


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
_smart_search_service = None

def get_smart_search_service():
    """–°–∏–Ω–≥–ª—Ç–æ–Ω –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–µ—Ä–≤–∏—Å—É —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    global _smart_search_service
    if _smart_search_service is None:
        _smart_search_service = SmartSearchService(get_es_client())
    return _smart_search_service


# –ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–º–µ—Ä–æ–≤ –¥–µ–ª –≤–Ω–µ –∫–ª–∞—Å—Å–∞ SmartSearchService
def extract_case_numbers_from_query(query: str) -> List[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä–∞ –¥–µ–ª –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤.
    
    Args:
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –Ω–æ–º–µ—Ä–æ–≤ –¥–µ–ª
    """
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–º–µ—Ä–æ–≤ –¥–µ–ª —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
    pattern = r'[–êA]\d{1,2}-\d+/\d{2,4}(?:-[–ê-–Ø–∞-—èA-Za-z0-9]+)*'
    
    # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–µ
    if isinstance(query, bytes):
        query = query.decode('utf-8')
            
    match = re.search(pattern, query)
    
    if not match:
        logger.info(f"–ù–æ–º–µ—Ä –¥–µ–ª–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∑–∞–ø—Ä–æ—Å–µ: '{query}'")
        return []
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π –Ω–æ–º–µ—Ä –¥–µ–ª–∞
    case_number = match.group(0)
    logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω –Ω–æ–º–µ—Ä –¥–µ–ª–∞: {case_number}")
    
    variants = [case_number]
    
    # –°–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –±—É–∫–≤–∞–º–∏ –ê/A
    if case_number.startswith("–ê"):  # —Ä—É—Å—Å–∫–∞—è –ê
        latin_variant = "A" + case_number[1:]
        variants.append(latin_variant)
        logger.info(f"–°–æ–∑–¥–∞–Ω –ª–∞—Ç–∏–Ω—Å–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç: {latin_variant}")
    elif case_number.startswith("A"):  # –ª–∞—Ç–∏–Ω—Å–∫–∞—è A
        russian_variant = "–ê" + case_number[1:]
        variants.append(russian_variant)
        logger.info(f"–°–æ–∑–¥–∞–Ω —Ä—É—Å—Å–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç: {russian_variant}")
    
    # –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    logger.info(f"–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω—ã –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞: {variants}")
    return variants



def search_law_chunks(query: str, top_n: int = 7) -> List[str]:
    """
    –ü–æ–∏—Å–∫ –≤ Elasticsearch –ø–æ –≤—Å–µ–º –∏–Ω–¥–µ–∫—Å–∞–º —Å —É—á–µ—Ç–æ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –Ω–æ–º–µ—Ä–æ–≤ –¥–µ–ª.
    
    Args:
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        top_n: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞
    """
    try:
        logger.info(f"üîç [ES] –ù–∞—á–∞–ª–æ –ø–æ–∏—Å–∫–∞ –≤ Elasticsearch –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
        es = get_es_client()
        
        # –ü—Ä—è–º–∞—è –ø–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –Ω–æ–º–µ—Ä –¥–µ–ª–∞
        logger.info(f"üîç [ES] –ò—â–µ–º –Ω–æ–º–µ—Ä –¥–µ–ª–∞ –Ω–∞–ø—Ä—è–º—É—é")
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–º–µ—Ä–æ–≤ –¥–µ–ª —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
        pattern = r'[–êA]\d{1,2}-\d+/\d{2,4}(?:-[–ê-–Ø–∞-—èA-Za-z0-9]+)*'
        match = re.search(pattern, query)
        
        if match:
            full_case_number = match.group(0)
            # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é —á–∞—Å—Ç—å –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞ (–±–µ–∑ —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤)
            base_parts = full_case_number.split('-')
            if len(base_parts) >= 2:
                court_num_and_case = base_parts[0] + '-' + base_parts[1]
                year_parts = base_parts[1].split('/')
                if len(year_parts) > 1:
                    base_case_number = f"{court_num_and_case}/{year_parts[1].split('-')[0]}"
                    logger.info(f"üîç [ES] –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞: {base_case_number}")
                else:
                    base_case_number = full_case_number
            else:
                base_case_number = full_case_number
            
            # –°–æ–∑–¥–∞–µ–º —Ä—É—Å—Å–∫–∏–π –∏ –ª–∞—Ç–∏–Ω—Å–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç—ã
            if base_case_number[0] == '–ê':  # –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∞—è –ê
                russian_version = base_case_number
                latin_version = 'A' + base_case_number[1:]
            else:  # –ª–∞—Ç–∏–Ω—Å–∫–∞—è A
                latin_version = base_case_number
                russian_version = '–ê' + base_case_number[1:]
                
            logger.info(f"üîç [ES] –í–∞—Ä–∏–∞–Ω—Ç—ã –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞ - –ø–æ–ª–Ω—ã–π: {full_case_number}, –±–∞–∑–æ–≤—ã–π: {base_case_number}")
            logger.info(f"üîç [ES] –í–∞—Ä–∏–∞–Ω—Ç—ã –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞ - —Ä—É—Å: {russian_version}, –ª–∞—Ç: {latin_version}")
            
            # –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –ø–æ –Ω–æ–º–µ—Ä—É –¥–µ–ª–∞ –≤–∫–ª—é—á–∞—è –ø–æ–ª–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∏ –æ—Å–Ω–æ–≤–Ω—É—é —á–∞—Å—Ç—å
            direct_query = {
                "query": {
                    "bool": {
                        "should": [
                            {"term": {"case_number": full_case_number}},
                            {"term": {"case_number": full_case_number.replace("–ê", "A")}},
                            {"term": {"case_number": full_case_number.replace("A", "–ê")}},
                            {"term": {"case_number": base_case_number}},
                            {"term": {"case_number": base_case_number.replace("–ê", "A")}},
                            {"term": {"case_number": base_case_number.replace("A", "–ê")}},
                            {"match_phrase": {"full_text": full_case_number}},
                            {"match_phrase": {"full_text": full_case_number.replace("–ê", "A")}},
                            {"match_phrase": {"full_text": full_case_number.replace("A", "–ê")}}
                        ],
                        "minimum_should_match": 1
                    }
                },
                "size": top_n,
                "sort": [
                    {"doc_id": {"order": "asc"}},
                    {"chunk_id": {"order": "asc"}}
                ]
            }
            
            try:
                logger.info(f"üîç [ES] –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –ø–æ –Ω–æ–º–µ—Ä—É –¥–µ–ª–∞: {json.dumps(direct_query, ensure_ascii=False)}")
                direct_result = es.search(index="court_decisions_index", body=direct_query)
                direct_hits = direct_result["hits"]["hits"]
                
                if direct_hits:
                    logger.info(f"üîç [ES] –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ —É—Å–ø–µ—à–µ–Ω! –ù–∞–π–¥–µ–Ω–æ {len(direct_hits)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                    
                    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    formatted_results = []
                    
                    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ doc_id
                    docs = {}
                    for hit in direct_hits:
                        source = hit["_source"]
                        doc_id = source.get("doc_id", "unknown")
                        
                        if doc_id not in docs:
                            docs[doc_id] = {
                                "parts": [],
                                "metadata": {
                                    "case_number": source.get("case_number", ""),
                                    "court_name": source.get("court_name", ""),
                                    "date": source.get("date", ""),
                                    "subject": source.get("subject", ""),
                                    "claimant": source.get("claimant", ""),
                                    "defendant": source.get("defendant", "")
                                }
                            }
                        
                        docs[doc_id]["parts"].append({
                            "chunk_id": source.get("chunk_id", 0),
                            "text": source.get("full_text", "")
                        })
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    for doc_id, doc in docs.items():
                        parts = sorted(doc["parts"], key=lambda x: x["chunk_id"])
                        metadata = doc["metadata"]
                        
                        result_text = f"–°—É–¥–µ–±–Ω–æ–µ –¥–µ–ª–æ ‚Ññ {metadata['case_number']}\n"
                        result_text += f"–°—É–¥: {metadata['court_name']}\n"
                        result_text += f"–î–∞—Ç–∞: {metadata['date']}\n"
                        result_text += f"–ü—Ä–µ–¥–º–µ—Ç —Å–ø–æ—Ä–∞: {metadata['subject']}\n"
                        result_text += f"–ò—Å—Ç–µ—Ü: {metadata['claimant']}\n"
                        result_text += f"–û—Ç–≤–µ—Ç—á–∏–∫: {metadata['defendant']}\n\n"
                        
                        full_text = ""
                        for part in parts:
                            full_text += part["text"]
                        
                        result_text += f"–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–µ—à–µ–Ω–∏—è:\n{full_text}"
                        formatted_results.append(result_text)
                    
                    logger.info(f"üîç [ES] –í–æ–∑–≤—Ä–∞—â–∞–µ–º {len(formatted_results)} –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                    return formatted_results
                else:
                    logger.warning(f"üîç [ES] –î–µ–ª–æ {full_case_number} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø—Ä–∏ –ø—Ä—è–º–æ–º –ø–æ–∏—Å–∫–µ")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä—è–º–æ–º –ø–æ–∏—Å–∫–µ: {str(e)}")
                logger.exception("Stacktrace:")
        
        # –ï—Å–ª–∏ –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ SmartSearchService
        logger.info(f"üîç [ES] –ü—Ä–æ–±—É–µ–º –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ SmartSearchService")
        smart_search = get_smart_search_service()
        case_number = smart_search.extract_case_number(query)
        company_name = smart_search.extract_company_name(query)
        
        if case_number:
            logger.info(f"üîç [ES] SmartSearchService –æ–±–Ω–∞—Ä—É–∂–∏–ª –Ω–æ–º–µ—Ä –¥–µ–ª–∞: {case_number}")
            search_results = smart_search.smart_search(query, top_n)
            
            if search_results["type"] == "case_number" and search_results["results"]:
                logger.info(f"üîç [ES] SmartSearchService –Ω–∞—à–µ–ª {len(search_results['results'])} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                formatted_results = []
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–µ–ª–∞
                case_number = search_results["query_entity"]
                results = search_results["results"]
                
                if results:
                    # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–µ–ª–∞
                    full_text_parts = []
                    metadata = None
                    
                    for result in sorted(results, key=lambda x: x.get("chunk_id", 0)):
                        if not metadata:
                            metadata = {
                                "case_number": result.get("case_number", ""),
                                "court_name": result.get("court_name", ""),
                                "date": result.get("date", ""),
                                "subject": result.get("subject", ""),
                                "claimant": result.get("claimant", ""),
                                "defendant": result.get("defendant", "")
                            }
                        
                        full_text_parts.append(result.get("full_text", ""))
                    
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    result_text = f"–°—É–¥–µ–±–Ω–æ–µ –¥–µ–ª–æ ‚Ññ {metadata['case_number']}\n"
                    result_text += f"–°—É–¥: {metadata['court_name']}\n"
                    result_text += f"–î–∞—Ç–∞: {metadata['date']}\n"
                    result_text += f"–ü—Ä–µ–¥–º–µ—Ç —Å–ø–æ—Ä–∞: {metadata['subject']}\n"
                    result_text += f"–ò—Å—Ç–µ—Ü: {metadata['claimant']}\n"
                    result_text += f"–û—Ç–≤–µ—Ç—á–∏–∫: {metadata['defendant']}\n\n"
                    result_text += f"–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–µ—à–µ–Ω–∏—è:\n{''.join(full_text_parts)}"
                    
                    formatted_results.append(result_text)
                
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                if formatted_results:
                    logger.info(f"üîç [ES] SmartSearchService: —É—Å–ø–µ—à–Ω–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(formatted_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                    return formatted_results
        
        # –ï—Å–ª–∏ –≤—Å–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–∏—Å–∫
        logger.info(f"üîç [ES] –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–∏—Å–∫")
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏
        court_decisions_limit = max(2, top_n // 4)
        court_reviews_limit = max(1, top_n // 4)
        legal_articles_limit = max(1, top_n // 4)
        ruslawod_chunks_limit = top_n - court_decisions_limit - court_reviews_limit - legal_articles_limit
        
        # 1. –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ court_decisions_index (—Å—É–¥–µ–±–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è)
        logger.info(f"üîç [ES] –í—ã–ø–æ–ª–Ω—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –≤ court_decisions_index")
        court_decision_results = search_court_decisions(es, query, court_decisions_limit)
        results = court_decision_results.copy() if court_decision_results else []
        
        # 2. –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ ruslawod_chunks_index (—á–∞–Ω–∫–∏ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞)
        logger.info(f"üîç [ES] –í—ã–ø–æ–ª–Ω—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –≤ ruslawod_chunks_index")
        ruslawod_chunks_results = search_ruslawod_chunks(es, query, ruslawod_chunks_limit)
        if ruslawod_chunks_results:
            results.extend(ruslawod_chunks_results)
        
        # 3. –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ court_reviews_index (–æ–±–∑–æ—Ä—ã —Å—É–¥–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π)
        logger.info(f"üîç [ES] –í—ã–ø–æ–ª–Ω—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –≤ court_reviews_index")
        court_reviews_results = search_court_reviews(es, query, court_reviews_limit)
        if court_reviews_results:
            results.extend(court_reviews_results)
        
        # 4. –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ legal_articles_index (–ø—Ä–∞–≤–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏)
        logger.info(f"üîç [ES] –í—ã–ø–æ–ª–Ω—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –≤ legal_articles_index")
        legal_articles_results = search_legal_articles(es, query, legal_articles_limit)
        if legal_articles_results:
            results.extend(legal_articles_results)
        
        logger.info(f"üîç [ES] –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–∏—Å–∫: –Ω–∞–π–¥–µ–Ω–æ –≤—Å–µ–≥–æ {len(results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return results
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ Elasticsearch: {str(e)}")
        logger.exception("Stacktrace:")
        return []
    

def search_court_decisions(es, query: str, limit: int) -> List[str]:
    """
    –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ court_decisions_index (—Å—É–¥–µ–±–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è).
    
    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        List[str]: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ü–†–ê–í–ò–õ–¨–ù–û–ï –∏–º—è –∏–Ω–¥–µ–∫—Å–∞ –Ω–∞–ø—Ä—è–º—É—é
        index_name = "court_decisions_index"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if not es.indices.exists(index=index_name):
            logger.warning(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
            return []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –¥–µ–ª–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
        case_number_pattern = r'[–êA]\d{1,2}-\d+/\d{2,4}(?:-[–ê-–Ø–∞-—èA-Za-z0-9]+)*'
        case_number_matches = re.findall(case_number_pattern, query)
        
        case_numbers = []
        for number in case_number_matches:
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –Ω–æ–º–µ—Ä
            case_numbers.append(number)
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç —Å –¥—Ä—É–≥–æ–π –±—É–∫–≤–æ–π (–ê/A)
            if '–ê' in number:
                case_numbers.append(number.replace('–ê', 'A'))
            else:
                case_numbers.append(number.replace('A', '–ê'))
        
        if case_numbers:
            logger.info(f"üîç [ES] –ò–∑–≤–ª–µ—á–µ–Ω—ã –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞: {case_numbers}")
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞
            should_clauses = []
            for case_number in case_numbers:
                should_clauses.append({
                    "term": {
                        "case_number": case_number
                    }
                })
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∞–∫–∂–µ –ø–æ–∏—Å–∫ –ø–æ –ø–æ–ª–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É –¥–ª—è –Ω–æ–º–µ—Ä–æ–≤ –¥–µ–ª
                should_clauses.append({
                    "match_phrase": {
                        "full_text": case_number
                    }
                })
            
            body = {
                "size": limit,
                "query": {
                    "bool": {
                        "should": should_clauses,
                        "minimum_should_match": 1
                    }
                }
            }
            logger.info(f"üîç [ES] –ü–æ–∏—Å–∫ –ø–æ –Ω–æ–º–µ—Ä—É –¥–µ–ª–∞: {json.dumps(should_clauses[:2], ensure_ascii=False)}")
        else:
            # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π multi_match
            body = {
                "size": limit,
                "query": {
                    "multi_match": {
                        "query": query,
                        "fields": [
                            "case_number^5",
                            "judges^3",
                            "claimant^3",
                            "defendant^3",
                            "subject^2",
                            "arguments",
                            "conclusion",
                            "full_text"
                        ],
                        "type": "best_fields",
                        "fuzziness": "AUTO"
                    }
                }
            }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Å–≤–µ—Ç–∫—É –¥–ª—è –ª—é–±–æ–≥–æ —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞
        body["highlight"] = {
            "fields": {
                "full_text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]}
            },
            "fragment_size": 300,
            "number_of_fragments": 3
        }
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        logger.info(f"üîç [ES] –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫: {json.dumps(body, ensure_ascii=False)[:200]}...")
        response = es.search(index=index_name, body=body)
        hits = response["hits"]["hits"]
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        if hits:
            found_case_numbers = [hit["_source"].get("case_number", "N/A") for hit in hits[:3]]
            logger.info(f"üîç [ES] –ù–∞–π–¥–µ–Ω–æ {len(hits)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ü–µ—Ä–≤—ã–µ –Ω–æ–º–µ—Ä–∞ –¥–µ–ª: {', '.join(found_case_numbers)}")
        else:
            logger.warning(f"üîç [ES] –ü–æ –∑–∞–ø—Ä–æ—Å—É '{query}' —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        results = []
        for hit in hits:
            source = hit["_source"]
            case_number = source.get("case_number", "")
            subject = source.get("subject", "")
            judges = source.get("judges", "")
            claimant = source.get("claimant", "")
            defendant = source.get("defendant", "")
            full_text = source.get("full_text", "")
            
            highlights = hit.get("highlight", {}).get("full_text", [])
            highlight_text = "...\n".join(highlights)
            
            result = f"–°—É–¥–µ–±–Ω–æ–µ –¥–µ–ª–æ ‚Ññ {case_number}\n"
            result += f"–°—É–¥—å–∏: {judges}\n–ò—Å—Ç–µ—Ü: {claimant}\n–û—Ç–≤–µ—Ç—á–∏–∫: {defendant}\n–ü—Ä–µ–¥–º–µ—Ç: {subject}\n"
            
            if highlights:
                result += f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n{highlight_text}\n\n"
            
            result += f"–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç:\n{full_text[:2000]}..."
            
            results.append(result)
        
        return results
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ court_decisions_index: {str(e)}")
        logger.exception("–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ—à–∏–±–∫–µ:")
        return []


def search_ruslawod_chunks(es, query: str, limit: int) -> List[str]:
    """
    –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ ruslawod_chunks_index (—á–∞–Ω–∫–∏ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞).
    
    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        List[str]: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        index_name = ES_INDICES.get("ruslawod_chunks", "ruslawod_chunks_index")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if not es.indices.exists(index=index_name):
            logger.warning(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
            return []
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        body = {
            "size": limit,
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": [
                        "text^3",
                        "content^3",
                        "full_text^2",
                        "law_name",
                        "section_name",
                        "article_name"
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            },
            "highlight": {
                "fields": {
                    "text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "content": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "full_text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]}
                },
                "fragment_size": 300,
                "number_of_fragments": 3
            }
        }
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        response = es.search(index=index_name, body=body)
        hits = response["hits"]["hits"]
        
        results = []
        for hit in hits:
            source = hit["_source"]
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
            law_name = source.get("law_name", "")
            section = source.get("section_name", source.get("section", ""))
            article = source.get("article_name", source.get("article", ""))
            content = source.get("content", source.get("text", source.get("full_text", "")))
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
            document_id = source.get("document_id", "")
            chunk_id = source.get("chunk_id", "")
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥—Å–≤–µ—á–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            highlights = []
            for field in ["text", "content", "full_text"]:
                if hit.get("highlight", {}).get(field):
                    highlights.extend(hit["highlight"][field])
            
            highlight_text = "...\n".join(highlights) if highlights else ""
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = f"–ó–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ: {law_name}\n"
            
            if section:
                result += f"–†–∞–∑–¥–µ–ª: {section}\n"
            
            if article:
                result += f"–°—Ç–∞—Ç—å—è: {article}\n"
            
            if document_id and chunk_id:
                result += f"ID –¥–æ–∫—É–º–µ–Ω—Ç–∞: {document_id}, ID —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞: {chunk_id}\n"
            
            if highlight_text:
                result += f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n{highlight_text}\n\n"
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            if content:
                content_preview = content[:2000] + "..." if len(content) > 2000 else content
                result += f"–¢–µ–∫—Å—Ç:\n{content_preview}"
            
            results.append(result)
        
        logger.info(f"üîç [ES] –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞")
        return results
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ ruslawod_chunks_index: {str(e)}")
        return []

def search_court_reviews(es, query: str, limit: int) -> List[str]:
    """
    –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ court_reviews_index (–æ–±–∑–æ—Ä—ã —Å—É–¥–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π).
    
    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        List[str]: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        index_name = ES_INDICES.get("court_reviews", "court_reviews_index")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if not es.indices.exists(index=index_name):
            logger.warning(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
            return []
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        body = {
            "size": limit,
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": [
                        "title^4",
                        "content^3",
                        "description^2",
                        "text",
                        "full_text"
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            },
            "highlight": {
                "fields": {
                    "title": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "content": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "full_text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]}
                },
                "fragment_size": 300,
                "number_of_fragments": 3
            }
        }
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        response = es.search(index=index_name, body=body)
        hits = response["hits"]["hits"]
        
        results = []
        for hit in hits:
            source = hit["_source"]
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
            title = source.get("title", "–û–±–∑–æ—Ä —Å—É–¥–µ–±–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏")
            content = source.get("content", source.get("text", source.get("full_text", "")))
            author = source.get("author", "")
            publication_date = source.get("publication_date", source.get("date", ""))
            source_name = source.get("source", "")
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥—Å–≤–µ—á–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            highlights = []
            for field in ["title", "content", "text", "full_text"]:
                if hit.get("highlight", {}).get(field):
                    highlights.extend(hit["highlight"][field])
            
            highlight_text = "...\n".join(highlights) if highlights else ""
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = f"–û–±–∑–æ—Ä —Å—É–¥–µ–±–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏: {title}\n"
            
            if author:
                result += f"–ê–≤—Ç–æ—Ä: {author}\n"
            
            if publication_date:
                result += f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {publication_date}\n"
            
            if source_name:
                result += f"–ò—Å—Ç–æ—á–Ω–∏–∫: {source_name}\n"
            
            if highlight_text:
                result += f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n{highlight_text}\n\n"
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            if content:
                content_preview = content[:2000] + "..." if len(content) > 2000 else content
                result += f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n{content_preview}"
            
            results.append(result)
        
        logger.info(f"üîç [ES] –ù–∞–π–¥–µ–Ω–æ {len(results)} –æ–±–∑–æ—Ä–æ–≤ —Å—É–¥–µ–±–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏")
        return results
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ court_reviews_index: {str(e)}")
        return []

def search_legal_articles(es, query: str, limit: int) -> List[str]:
    """
    –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ legal_articles_index (–ø—Ä–∞–≤–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏).
    
    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        List[str]: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        index_name = ES_INDICES.get("legal_articles", "legal_articles_index")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if not es.indices.exists(index=index_name):
            logger.warning(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
            return []
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        body = {
            "size": limit,
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": [
                        "title^4",
                        "content^3",
                        "description^2",
                        "body",
                        "text"
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            },
            "highlight": {
                "fields": {
                    "title": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "content": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "body": {"pre_tags": ["<b>"], "post_tags": ["</b>"]},
                    "text": {"pre_tags": ["<b>"], "post_tags": ["</b>"]}
                },
                "fragment_size": 300,
                "number_of_fragments": 3
            }
        }
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        response = es.search(index=index_name, body=body)
        hits = response["hits"]["hits"]
        
        results = []
        for hit in hits:
            source = hit["_source"]
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
            title = source.get("title", "–ü—Ä–∞–≤–æ–≤–∞—è —Å—Ç–∞—Ç—å—è")
            content = source.get("content", source.get("body", source.get("text", "")))
            author = source.get("author", "")
            publication_date = source.get("publication_date", source.get("date", ""))
            source_name = source.get("source", "")
            tags = source.get("tags", source.get("keywords", ""))
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥—Å–≤–µ—á–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            highlights = []
            for field in ["title", "content", "body", "text"]:
                if hit.get("highlight", {}).get(field):
                    highlights.extend(hit["highlight"][field])
            
            highlight_text = "...\n".join(highlights) if highlights else ""
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = f"–ü—Ä–∞–≤–æ–≤–∞—è —Å—Ç–∞—Ç—å—è: {title}\n"
            
            if author:
                result += f"–ê–≤—Ç–æ—Ä: {author}\n"
            
            if publication_date:
                result += f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {publication_date}\n"
            
            if source_name:
                result += f"–ò—Å—Ç–æ—á–Ω–∏–∫: {source_name}\n"
            
            if tags:
                tags_str = tags if isinstance(tags, str) else ", ".join(tags)
                result += f"–¢–µ–≥–∏: {tags_str}\n"
            
            if highlight_text:
                result += f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n{highlight_text}\n\n"
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            if content:
                content_preview = content[:2000] + "..." if len(content) > 2000 else content
                result += f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n{content_preview}"
            
            results.append(result)
        
        logger.info(f"üîç [ES] –ù–∞–π–¥–µ–Ω–æ {len(results)} –ø—Ä–∞–≤–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π")
        return results
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ legal_articles_index: {str(e)}")
        return []

def create_court_decisions_index(es):
    """
    –°–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å court_decisions –≤ Elasticsearch, –µ—Å–ª–∏ –æ–Ω –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
    
    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
    """
    index_name = ES_INDICES.get("court_decisions", "court_decisions_index")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
    if not es.indices.exists(index=index_name):
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞
        mappings = {
            "properties": {
                "case_number": {"type": "text", "analyzer": "russian"},
                "judges": {"type": "text", "analyzer": "russian"},
                "claimant": {"type": "text", "analyzer": "russian"},
                "defendant": {"type": "text", "analyzer": "russian"},
                "subject": {"type": "text", "analyzer": "russian"},
                "arguments": {"type": "text", "analyzer": "russian"},
                "conclusion": {"type": "text", "analyzer": "russian"},
                "full_text": {"type": "text", "analyzer": "russian"}
            }
        }
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
        es.indices.create(
            index=index_name,
            body={
                "settings": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "analysis": {
                        "analyzer": {
                            "russian": {
                                "tokenizer": "standard",
                                "filter": ["lowercase", "russian_morphology", "russian_stop"]
                            }
                        }
                    }
                },
                "mappings": mappings
            }
        )
        
        logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å {index_name} —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω.")
    else:
        logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å {index_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")

def create_ruslawod_chunks_index(es):
    """
    –°–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å ruslawod_chunks_index –≤ Elasticsearch, –µ—Å–ª–∏ –æ–Ω –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
    
    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch
    """
    index_name = ES_INDICES.get("ruslawod_chunks", "ruslawod_chunks_index")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
    if not es.indices.exists(index=index_name):
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞
        mappings = {
            "properties": {
                "law_name": {"type": "text", "analyzer": "russian"},
                "section_name": {"type": "text", "analyzer": "russian"},
                "article_name": {"type": "text", "analyzer": "russian"},
                "document_id": {"type": "keyword"},
                "chunk_id": {"type": "keyword"},
                "content": {"type": "text", "analyzer": "russian"},
                "text": {"type": "text", "analyzer": "russian"},
                "full_text": {"type": "text", "analyzer": "russian"}
            }
        }
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
        es.indices.create(
            index=index_name,
            body={
                "settings": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "analysis": {
                        "analyzer": {
                            "russian": {
                                "tokenizer": "standard",
                                "filter": ["lowercase", "russian_morphology", "russian_stop"]
                            }
                        }
                    }
                },
                "mappings": mappings
            }
        )
        
        logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å {index_name} —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω.")
    else:
        logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å {index_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")

def create_indices():
    """
    –°–æ–∑–¥–∞–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–Ω–¥–µ–∫—Å—ã –≤ Elasticsearch.
    """
    try:
        es = get_es_client()
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
        create_court_decisions_index(es)
        create_ruslawod_chunks_index(es)
        
        # –¢–∞–∫–∂–µ –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è court_reviews_index –∏ legal_articles_index
        # –Ω–æ –ø–æ–∫–∞ –æ—Å—Ç–∞–≤–∏–º –∏—Ö –±–µ–∑ —è–≤–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è
        
        logger.info("‚úÖ –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–Ω–¥–µ–∫—Å—ã –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –∏–ª–∏ —Å–æ–∑–¥–∞–Ω—ã.")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–æ–≤: {str(e)}")

def index_court_decisions():
    """
    –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å—É–¥–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –≤ Elasticsearch.
    –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π.
    """
    create_indices()

def update_court_decisions_mapping(es=None):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ —Å—É–¥–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π, –¥–æ–±–∞–≤–ª—è—è –ø–æ–¥–¥–µ—Ä–∂–∫—É
    –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –Ω–æ–º–µ—Ä—É –¥–µ–ª–∞ –∏ –∫–æ–º–ø–∞–Ω–∏–∏.
    
    Args:
        es: –ö–ª–∏–µ–Ω—Ç Elasticsearch (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    if es is None:
        es = get_es_client()
    
    try:
        index_name = ES_INDICES.get("court_decisions", "court_decisions_index")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if not es.indices.exists(index=index_name):
            logger.warning(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å {index_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
            return False
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞
        mapping_update = {
            "properties": {
                "case_number": { 
                    "type": "keyword",
                    "fields": {
                        "text": { "type": "text", "analyzer": "russian" }
                    }
                },
                "defendant": { 
                    "type": "text", 
                    "analyzer": "russian",
                    "fields": {
                        "keyword": { "type": "keyword" }
                    }
                },
                "claimant": { 
                    "type": "text", 
                    "analyzer": "russian",
                    "fields": {
                        "keyword": { "type": "keyword" }
                    }
                },
                "doc_id": { "type": "keyword" },
                "chunk_id": { "type": "integer" }
            }
        }
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥
        es.indices.put_mapping(
            index=index_name,
            body=mapping_update
        )
        
        logger.info(f"‚úÖ –ú–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {index_name} —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω.")
        return True
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –º–∞–ø–ø–∏–Ω–≥–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {index_name}: {str(e)}")
        return False

def update_all_mappings():
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –º–∞–ø–ø–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.
    """
    try:
        es = get_es_client()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
        success = update_court_decisions_mapping(es)
        
        # –î–æ–±–∞–≤—å—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥—Ä—É–≥–∏—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        
        return success
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –º–∞–ø–ø–∏–Ω–≥–æ–≤: {str(e)}")
        return False

if __name__ == "__main__":
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –º–æ–¥—É–ª—è –Ω–∞–ø—Ä—è–º—É—é
    create_indices()
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥–∏ –¥–ª—è —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    update_all_mappings()
    
    # –ü—Ä–∏–º–µ—Ä –ø–æ–∏—Å–∫–∞
    test_query = "–ê65-28469/2012"
    results = search_law_chunks(test_query)
    print(f"–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{test_query}': –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    if results:
        print("\n–ü–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(results[0][:500] + "...")